[
  {
    "objectID": "posts/post-with-code/mlb-market-ranking/index.html",
    "href": "posts/post-with-code/mlb-market-ranking/index.html",
    "title": "MLB - Market Ranking",
    "section": "",
    "text": "2 Introduction\nThis will be a little bit of different type of post than I normally do. Most of posts on this blog are related to fish and/or aquatic sciences and not sports but I do thoroughly enjoy playing and watching sports, especially baseball. I grew up playing baseball, second base and shortstop, and have fond memories of my dad, younger brother and myself sitting around our tiny TV watching the greats of MLB in the 1990s and 2000s. Every October we all chat about post-season baseball and the thrill that it is. I could go on and on about my love for the sport, specifically the post-season, but lets get to the point of this post.\n\n\n3 MLB 2024 Post-Season\nThe 2024 post-season has been spectacular. There have been so many great stories and games, it has really been a treat to watch. On top of that we have the two best teams, the Yankees and the Dodgers, in the World Series with some of the best players in the league and yet people are mad and upset by this.\nWhy?\nSure it is nice to have an underdog to root for or teams that do not often make it, like my Minnesota Twins, but at the same time how cool is that we get two juggernauts who haven’t played each other in over 40 years face off in the World Series! On top of that, the only thing that would make this World Series even more exciting is if Ohtani was pitching!\nWith this being said, there has been a lot of talk about can you buy yourself a World Series considering these two teams often have the high payrolls and whether that is a good or bad thing.\nWith this question in hand, everyone’s favourite baseball guy, Jimmy aka Jomboy set to the task of answering just that. He put out this great video on Talkin’ Baseball, where he showed from 1989 to 2024 the two teams that made it to the world series, which team won and which lost, and their market rank, with 1 being the team that spent the most money and 30 being the team that spent the lowest amount of money. He did this in a table which was great, however, as a scientist and someone who loves looking at visualizations, I figured I could make some figures to complement this video and the story Jimmy is providing. So lets visualize this data.\n\n\n4 Market Ranking of World Series Teams"
  },
  {
    "objectID": "posts/post-with-code/nichetools/index.html",
    "href": "posts/post-with-code/nichetools/index.html",
    "title": "How to use {nichetools} with nicheROVER",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to use {nicheROVER} and {nichetools} to extract and then visualize estimates of trophic niche size and similarities for multiple freshwater fish using {ggplot2}.\nThis vignette can be used for additional purposes including estimating niche size and similarities among different groups of aquatic and/or terrestrial species. Furthermore, niche size and similarities for different behaviours exhibited within a population can be made using behavioural data generated from acoustic telemetry (e.g., differences in habitat occupancy).\n\n\n0.2 Bring in trophic niche data\nFirst we will load the necessary packages to preform the analysis and visualization. We will use {nicheROVER} and {nichetools} to preform the analysis. We will use {dplyr}, {tidyr}, and {purrr} to manipulate data and iterate processes. Lastly, we will use {ggplot2}, {ggtext}, and {patchwork} to plot, add labels, and arrange plots.\nI will add that many of the {dplyr} and {tidyr} functions and processes can be replaced using {data.table} which is great when working with large data sets.\n\n{\n  library(dplyr)\n  library(ggplot2)\n  library(ggtext)\n  library(ggh4x)\n  library(nicheROVER) \n  library(nichetools)\n  library(patchwork)\n  library(purrr)\n  library(stringr)\n  library(tidyr)\n}\n\nWarning: package 'nichetools' was built under R version 4.4.1\n\n\nFor the purpose of the vignette we will be using the fish data frame that is available within {nicheROVER}. We will remove \\(\\delta\\)34S for simplicity of the vignette. If more than two isotopes or metrics are being used to compare niche sizes and similarities, please use the functions for each pairing. Right now some functions (i.e., niche_ellipse()) in {nichetools} doesn’t have the ability to work with more than two isotopes. This will become a feature at some point but for now. Please be patient and use the functions for each pairing you have.\nWe will first use the function janitor::clean_names() to clean up column names. For your purposes you will need to replace fish with your data frame either by loading a csv, rds, or qs, with your data. You can do this multiple ways, I prefer using readr::read_csv() but base R’s read.csv() works perfectly fine.\n\ndf &lt;- fish %&gt;% \n  janitor::clean_names()\n\nIf there are any isotopic values that did not run and are NA, they will need to be removed because {nicheROVER}’s functions will not accommodate values of NA.\n\n\n0.3 Estimate posterior distribution with Normal-Inverse-Wishart (NIW) priors.\nWe will take 1,000 posterior samples for each group. You can change this but suggest nothing less than 1,000.\n\nnsample &lt;- 1000\n\nWe will then split the data frame into a list with each species as a data frame object within the list, We will then iterate over the list, using map() from {purrr}, to estimate posterior distribution using Normal-Inverse-Wishart (NIW) priors.\n\nfish_par &lt;- df %&gt;% \n  split(.$species) %&gt;% \n  map(~ select(., d13c, d15n)) %&gt;% \n  map(~ niw.post(nsample = nsample, X = .))\n\n\n\n0.4 Extract μ values\nWe will use extract_mu()to extract posteriors for \\(\\mu\\) estimates. The default output of extract_mu() is long format which works for plotting with {ggplot2} and other functions in {nichetools}. If we want wide format we can specify the argument format with \"wide\", however, it is unlikely you will need this data in wide format.\n\ndf_mu &lt;- extract_mu(fish_par)\n\nThe default output will be lacking some info for plotting. We will need to add in a column that is the element abbreviation and neutron number to be used in axis labeling.\n\ndf_mu &lt;- df_mu %&gt;%\n  mutate(\n    element = case_when(\n      isotope == \"d15n\" ~ \"N\",\n      isotope == \"d13c\" ~ \"C\",\n    ), \n    neutron = case_when(\n      isotope == \"d15n\" ~ 15,\n      isotope == \"d13c\" ~ 13,\n    ) \n  )\n\n\n\n0.5 Extract Σ values\nWe will use extract_sigma() to extract posterior estimates for \\(\\Sigma\\). The default output of extract_sigma() is wide format which doesn’t work for plotting with {ggplot2} but does work other functions in {nichetools}. If we want long for plotting we can specify the argument format with \"long\".\n\ndf_sigma &lt;- extract_sigma(fish_par)\n\nFor plotting we will need the extracted \\(\\Sigma\\) values to be in long format. We also need to remove \\(\\Sigma\\) values for when the both isotope columns are the same isotope.\n\ndf_sigma_cn &lt;- extract_sigma(fish_par, \n                             data_format = \"long\") %&gt;%\n  filter(id != isotope)\n\n\n\n0.6 Plot posterior distrubtion of μ and Σ\nFor most plotting within this vignette, I will split() the data frame by isotope, creating a list that I will then use imap() to iterate over the list to create plots. We will use geom_density() to represent densities for both \\(\\mu\\) and \\(\\Sigma\\). Plot objects will then be stored in a list.\nFirst we will plot \\(\\mu\\) for each isotope. We will use {patchwork} to configure plots for multi-panel figures. This package is phenomenal and uses math operators to configure and manipulate the plots to create multi-panel figures.\nFor labeling we are also going to use element_markdown() from {ggtext} to work with the labels that are needed to correctly display the isotopic signature. If you are working other data please replace.\n\nposterior_plots &lt;- df_mu %&gt;%\n  split(.$isotope) %&gt;%\n  imap(\n    ~ ggplot(data = ., aes(x = mu_est)) +\n      geom_density(aes(fill = sample_name), alpha = 0.5) +\n      scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                           option = \"D\", name = \"Species\") +\n      theme_bw() +\n      theme(panel.grid = element_blank(),\n            axis.title.x =  element_markdown(),\n            axis.title.y =  element_markdown(),\n            legend.position = \"none\",\n            legend.background = element_blank()\n      ) +\n      labs(\n        x = paste(\"\\u00b5&lt;sub&gt;\\U03B4&lt;/sub&gt;\", \"&lt;sub&gt;&lt;sup&gt;\",\n                  unique(.$neutron), \"&lt;/sup&gt;&lt;/sub&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element), \"&lt;/sub&gt;\", sep = \"\"),\n        y = paste0(\"p(\\u00b5 &lt;sub&gt;\\U03B4&lt;/sub&gt;\",\"&lt;sub&gt;&lt;sup&gt;\",\n                   unique(.$neutron), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                   \"&lt;sub&gt;\",unique(.$element),\"&lt;/sub&gt;\",\n                   \" | X)\"), sep = \"\")\n  )\n\nposterior_plots$d15n +\n  theme(legend.position = c(0.18, 0.82)) + \n  posterior_plots$d13c\n\n\n\n\n\n\n\n\nFor labeling purposes we need to add columns that are the element abbreviation and neutron number. I do this by using case_when() which are vectorized if else statements.\n\ndf_sigma_cn &lt;- df_sigma_cn %&gt;%\n  mutate(\n    element_id = case_when(\n      id == \"d15n\" ~ \"N\",\n      id == \"d13c\" ~ \"C\",\n    ),\n    neutron_id = case_when(\n      id == \"d15n\" ~ 15,\n      id == \"d13c\" ~ 13,\n    ),\n    element_iso = case_when(\n      isotope == \"d15n\" ~ \"N\",\n      isotope == \"d13c\" ~ \"C\",\n    ),\n    neutron_iso = case_when(\n      isotope == \"d15n\" ~ 15,\n      isotope == \"d13c\" ~ 13,\n    )\n  )\n\nNext we will plot the posteriors for \\(\\Sigma\\).\n\nsigma_plots &lt;- df_sigma_cn %&gt;%\n  group_split(id, isotope) %&gt;%\n  imap(\n    ~ ggplot(data = ., aes(x = post_sample)) +\n      geom_density(aes(fill = sample_name), alpha = 0.5) +\n      scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                           option = \"D\", name = \"Species\") +\n      theme_bw() +\n      theme(panel.grid = element_blank(),\n            axis.title.x =  element_markdown(),\n            axis.title.y =  element_markdown(),\n            legend.position = \"none\"\n      ) +\n      labs(\n        x = paste(\"\\U03A3\",\"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_id), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_id),\"&lt;/sub&gt;\",\" \",\n                  \"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_iso), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_iso),\"&lt;/sub&gt;\", sep = \"\"),\n        y = paste(\"p(\", \"\\U03A3\",\"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_id), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_id),\"&lt;/sub&gt;\",\" \",\n                  \"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_iso), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_iso),\"&lt;/sub&gt;\", \" | X)\", sep = \"\"),\n      )\n  )\n\nsigma_plots[[1]] + \n  theme(legend.position = c(0.1, 0.82))\n\n\n\n\n\n\n\n\n\n\n0.7 Estimate niche ellipse\nWe then will use niche_ellipse() to easily extract ellipse for each \\(\\Sigma\\) estimate (i.e., 1000). If you are to have additional isotopes or metrics, you will need to create mu and sigma objects for each pairing, as currently this function only handles two isotopes. In the future, there likely will be the ability to specify the number of isotopes you have with the default being two. The reason for the lack of functionality is ellipse::ellipse() can only work within two-dimensions, not three, so you will have to create multiple ellipse() calls for each combination of isotopes or metrics and I haven’t had the time to implement this. The function will also tell you how long it took to process as with large sets of isotope data it is nice to know the time it takes for the function to work.\n\nellipse_df &lt;- niche_ellipse(dat_mu = df_mu, dat_sigma = df_sigma)\n\n→ Total time processing was 0.02 secs\n\n\nWe will randomly sample 10 ellipses out of 1,000. You can change this but this seems pretty standard.\n\nset.seed(14)\n\nrandom_ellipse &lt;- ellipse_df %&gt;% \n  group_by(sample_name, sample_number) %&gt;% \n  nest() %&gt;%\n  group_by(sample_name) %&gt;% \n  slice_sample(n = 10, replace = TRUE) %&gt;% \n  ungroup() %&gt;% \n  unnest(cols = c(data))  \n\n\n\n0.8 Plot ellipses, densities of each istope, and isotope biplot\nWe will first plot the ellipse for each sample_name\n\nellipse_plots &lt;- ggplot() + \n  geom_polygon(data = random_ellipse,\n               mapping = aes(x = d13c, y = d15n,\n                             group = interaction(sample_number, sample_name),\n                             color = sample_name),\n               fill = NA,\n               linewidth = 0.5) + \n  \n  scale_colour_viridis_d(begin = 0.25, end = 0.75, \n                         option = \"D\", name = \"species\",\n  ) + \n  scale_x_continuous(breaks = rev(seq(-20, -40, -2))) +\n  scale_y_continuous(breaks = seq(6, 16, 2)) +\n  theme_bw(base_size = 10) +\n  theme(axis.text = element_text(colour = \"black\"),\n        panel.grid = element_blank(), \n        legend.position = \"none\", \n        legend.title = element_text(hjust = 0.5),\n        legend.background = element_blank()) + \n  labs(x = expression(paste(delta ^ 13, \"C\")), \n       y = expression(paste(delta ^ 15, \"N\")))\n\nWe need to turn df into long format to iterate over using imap() to easily create density plots. You will notice that I again use case_when() to make columns of element abbreviations and neutron numbers that will be used in plot labeling.\n\niso_long &lt;- df %&gt;%\n  pivot_longer(cols = -species,\n               names_to = \"isotope\", \n               values_to = \"value\") %&gt;% \n  mutate(\n    element = case_when(\n      isotope == \"d15n\" ~ \"N\",\n      isotope == \"d13c\" ~ \"C\",\n    ), \n    neutron = case_when(\n      isotope == \"d15n\" ~ 15,\n      isotope == \"d13c\" ~ 13,\n    )\n  )\n\nWe will then make density plots for each isotope using geom_density()\n\niso_density &lt;- iso_long %&gt;% \n  group_split(isotope) %&gt;% \n  imap(\n    ~ ggplot(data = .) + \n      geom_density(aes(x = value, \n                       fill = species), \n                   alpha = 0.35, \n                   linewidth = 0.8) +\n      scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                           option = \"D\", name = \"Species\") +\n      theme_bw(base_size = 10) +\n      theme(axis.text = element_text(colour = \"black\"),\n            panel.grid = element_blank(), \n            legend.position = c(0.15, 0.55), \n            legend.background = element_blank(), \n            axis.title.x = element_markdown(family = \"sans\")) + \n      labs(x =  paste(\"\\U03B4\",\n                      \"&lt;sup&gt;\", unique(.$neutron), \"&lt;/sup&gt;\",unique(.$element), \n                      sep = \"\"), \n           y = \"Density\")\n  )\n\nd13c_density &lt;- iso_density[[1]] + \n  scale_x_continuous(breaks = rev(seq(-20, -34, -2)),\n                     limits = rev(c(-20, -34)))\n\nd15n_density &lt;- iso_density[[2]] +\n  scale_x_continuous(breaks = seq(5, 15, 2.5), \n                     limits = c(5, 15)) + \n  theme(\n    legend.position = \"none\"\n  )\n\nLastly we will use geom_point() to make isotopic biplot.\n\niso_biplot &lt;- ggplot() + \n  geom_point(data = df, aes(x = d13c, y = d15n,\n                            fill = species),\n             shape = 21, colour = \"black\", \n             stroke = 0.8,\n             size = 3, alpha = 0.70) +\n  scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                       option = \"D\", name = \"species\") +\n  scale_x_continuous(breaks = rev(seq(-20, -39, -1))) +\n  scale_y_continuous(breaks = seq(5, 17, 1)) +\n  theme_bw(base_size = 10) +\n  theme(axis.text = element_text(colour = \"black\"),\n        panel.grid = element_blank(), \n        legend.position = \"none\", \n        legend.background = element_blank()) + \n  labs(x = expression(paste(delta ^ 13, \"C\")), \n       y = expression(paste(delta ^ 15, \"N\")))\n\n\n\n0.9 Use {patchwork} to make ellipse, density, and biplots into a paneled figure.\nWe can also use the function plot_annotation() to add lettering to the figure that can be used in the figure description. To maneuver where plot_annotation() places the lettering, we need to add plot.tag.position = c(x, y) to the theme() call in every plot.\n\nd13c_density + ellipse_plots + iso_biplot + d15n_density +\n  plot_annotation(tag_levels = \"a\", \n                  tag_suffix = \")\")\n\n\n\n\n\n\n\n\n\n\n0.10 Determine the 95% niche similarties for each species\nWe will use the overlap() function from {nicheROVER} to estimate the percentage of similarity among species. We will set overlap to assess based on 95% similarities.\n\nover_stat &lt;- overlap(fish_par, nreps = nsample, nprob = 1000, \n                     alpha = 0.95)\n\nWe then are going transform this output to a data frame using extract_overlap() plotting so we can assess overall similarities among species.\n\nover_stat_df &lt;- extract_overlap(data = over_stat) %&gt;% \n    mutate(\n      niche_overlap_perc = niche_overlap * 100\n  )\n\nWe then are going to take our newly made data frame and extract out the mean percentage of similarities and the 2.5% and 97.5% quarantines. We plot these as lines and dotted lines on our percent similarity density figure.\n\nover_sum &lt;- over_stat_df %&gt;% \n  group_by(sample_name_a, sample_name_b) %&gt;% \n  summarise(\n    mean_niche_overlap = round(mean(niche_overlap_perc), digits = 2),\n    qual_2.5 = round(quantile(niche_overlap_perc, probs = 0.025, na.rm = TRUE), digits = 2), \n    qual_97.5 = round(quantile(niche_overlap_perc, probs = 0.975, na.rm = TRUE), digits = 2)\n  ) %&gt;% \n  ungroup() %&gt;% \n  pivot_longer(cols = -c(sample_name_a, sample_name_b, mean_niche_overlap), \n               names_to = \"percentage\", \n               values_to = \"niche_overlap_qual\") %&gt;% \n  mutate(\n    percentage = as.numeric(str_remove(percentage, \"qual_\"))\n  ) \n\nWe are now going to use ggplot(), geom_density(), and fact_grid2() from {ggh4x}.\n\nggplot(data = over_stat_df, aes(x = niche_overlap_perc)) + \n  geom_density(aes(fill = sample_name_a)) + \n  geom_vline(data = over_sum, aes(xintercept = mean_niche_overlap), \n             colour = \"black\", linewidth = 1) +\n  geom_vline(data = over_sum, aes(xintercept = niche_overlap_qual), \n             colour = \"black\", linewidth = 1, linetype = 6) +\n  scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                       option = \"D\", name = \"Species\", \n                       alpha = 0.35) + \n  facet_grid2(sample_name_a ~ sample_name_b, \n                     independent = \"y\",\n                     scales = \"free_y\") + \n  theme_bw() + \n  theme(\n    panel.grid = element_blank(), \n    axis.text = element_text(colour = \"black\"), \n    legend.background = element_blank(),\n    strip.background = element_blank()\n  ) +\n  labs(x = paste(\"Overlap Probability (%)\", \"\\u2013\", \n                 \"Niche Region Size: 95%\"), \n       y = \"p(Percent Overlap | X)\")\n\n\n\n\n\n\n\n\n\n\n0.11 Estimate overall niche size\nWe are now going to estimate the overall size of the niche for each posterior sample by using the function extract_niche_size() which is a wrapper around niche.size() and some data manipulation functions.\n\nniche_size &lt;- extract_niche_size(fish_par)\n\nWe can calculate the mean niche size, standard deviation, and standard error.\n\nniche_size_mean &lt;- niche_size %&gt;% \n  group_by(sample_name) %&gt;% \n  summarise(\n    mean_niche = round(mean(niche_size), digits = 2), \n    sd_niche = round(sd(niche_size), digits = 2), \n    sem_niche = round(sd(niche_size) / sqrt(n()), digits = 2)\n  )\n\n\n\n0.12 Plot niche size\nWe will now use geom_violin(), geom_point(), and geom_errorbar() to plot the distribution for niche size for each species.\n\nggplot(data = niche_size) + \n  geom_violin(\n    aes(x = sample_name, y = niche_size),\n    width = 0.2) + \n  geom_point(data = niche_size_mean, aes(x = sample_name, y = mean_niche)) +\n  geom_errorbar(data = niche_size_mean, aes(x = sample_name, \n                                            ymin = mean_niche  - sem_niche, \n                                            ymax = mean_niche  + sem_niche), \n                width = 0.05) +\n  theme_bw(base_size = 15) + \n  theme(panel.grid = element_blank(), \n        axis.text = element_text(colour = \"black\")) + \n  labs(x = \"Species\", \n       y = \"Niche Size\") \n\n\n\n\n\n\n\n\nNow that we have our niche sizes and similarities determined we can make inferences about the species, trophic similarities, and the ecosystem."
  },
  {
    "objectID": "posts/post-with-code/shortest-path-gidstance/shortest_path_example_gdistance.html",
    "href": "posts/post-with-code/shortest-path-gidstance/shortest_path_example_gdistance.html",
    "title": "Shortest Paths Within a Boundary - {gdistance}",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to create the shortest distance among acoustic telemetry receivers within a confined boundary such as a lake, river, delta, or oceanscape. This workflow can be adapted to find the distance between any two points within a confined boundary.\nYou can download and unzip this vignette using the following code:\n\ninstall.packages(\"usethis\")\nusethis::use_course(\"https://github.com/benjaminhlina/shortest_path_example/archive/refs/heads/master.zip\")\n\n\n\n0.2 Disclaimer\nThis vignette uses {gdistance}, {raster}, and {sp} which as of October 2023 were retired. Please use the vignette using {pathroutr} for the timing being until I can update this vignette with using {terra}.\n\n\n0.3 Load shapefile and receiver locations\nWe will first load all the packages we need, we will use {gdistance} to find the shortest paths, {sf} to find the distances of those shortest paths.\n\n# ---- load packages ----\n{\n  library(dplyr)\n  library(gdistance)\n  library(ggplot2)\n  library(here)\n  library(purrr)\n  library(raster)\n  library(readr)\n  library(sf)\n  library(sp)\n  library(tibble)\n  library(tidyr)\n  make_line &lt;- function(lon, lat, llon, llat) {\n    st_linestring(matrix(c(lon, llon, lat, llat), 2, 2))\n  }\n}\n\nWe will bring bring in our shapefile. This vignette will use Big Sissabagama Lake as it is the lake I grew up fishing on in Wisconsin, USA. Please replace with the shapefile of your desired body of water.\n\nlake &lt;- st_read(dsn = here(\"Data\",\n                           \"shapefile\",\n                           \".\"),\n                layer = \"sissabagama_lake\")\n\nImportant that you convert to the correct UTM zone. For the vignette we are using UTM zone 15 N. Adjust your UTM zone accordingly.\n\nlake_utm &lt;- st_transform(lake, crs = 32615)\n\nCreate SpatialPloygonDataFrame we will use it to create a raster that will be a transition layer for paths to move across. We will use lake_utm as we need our raster layer in UTMs.\n\nlake_spd &lt;- as_Spatial(lake_utm)\n\nWe will then bring in our receiver locations. Replace rl_sum_sf with your receiver locations as a RDS or csv file type or whatever you use to document receiver locations.\n\nrl_sum_sf &lt;- read_rds(here(\"Data\",\n                           \"receiver locations\",\n                           \"rl_sum_sf.rds\"))\n\nConvert to UTMs for plotting purposes and make sure you use the correct UTM zone.\n\nrl_sum_utm &lt;- st_transform(rl_sum_sf, crs = 32615)\n\n\n\n0.4 Rasterize shapefile\nWe will look at lake SpatialPointsDataFrame via plot, then determine the boundary box (bbox) and save it as an object named ext.\n\nplot(lake_spd)\n\n# determine the extent of the SpatialPointsDataFrame\next &lt;- extent(lake_spd)\n\n\nThen we will create the raster, it is important here to control the res argument as that will result in varied resolution. For the vignette I used a resolution of 5 which represents 5 m since we are using UTMs. Using a more fine-scale resolution such as 5 m can be computationally intensive so for large systems scale this value up.\n\ns &lt;- raster(lake_spd, res = 5)\n# remove and change NA values to fit within the extent\ns &lt;- rasterize(x = lake_spd, y = s, field = 1)\n\n# plot raster to make sure it looks appropriate\nplot(s)\n\n\nThe last step is to create the transition layer. Directions will be queens move of 16 spaces. If in a larger systems direction could be reduced from queens space to rook or king, 4 or 8 to reduce computational complexity and speed.\n\ntrans &lt;- transition(x = s, transitionFunction = mean, directions = 16)\n\n\n\n0.5 Create every combination of paths for every receiver\nFirst we will convert receiver location which is a sf object to a tibble with each location combination.\n\nprep_path &lt;- rl_sum_sf %&gt;%\n  mutate(\n    lon = st_coordinates(.)[,\"X\"],# grab lon\n    lat = st_coordinates(.)[,\"Y\"],# grab lat\n  ) %&gt;%  \n  st_drop_geometry() %&gt;% # drop sf \n  # once geometry removed create to and from lat longs \n  mutate(llon = lon,\n         llat = lat,\n         lonlat = paste0(lon, \",\", lat),\n         llonllat = paste0(llon, \",\", llat)) %&gt;%\n  dplyr::select(-lon, -lat, -llon, -llat) %&gt;%\n  expand(lonlat, llonllat) %&gt;% # expand for each to and from combo \n  separate(lonlat, c(\"lon\", \"lat\"), \",\") %&gt;%\n  separate(llonllat, c(\"llon\", \"llat\"), \",\")\n\nprep_path has all of the path combinations but we lose the names of the receivers and which paths go from one receiver to another. We are going to add that information back in by creating an object called rec_order\n\nrec_order &lt;- prep_path %&gt;%\n  left_join( \n    rl_sum_sf %&gt;% \n      mutate(\n        lon = st_coordinates(.)[,\"X\"], # grab lon \n        lat = st_coordinates(.)[,\"Y\"]  # grab lat \n      ) %&gt;% \n      st_drop_geometry() %&gt;% # remove sf \n      rename(from = rec_name) %&gt;%  # Line up from names \n      dplyr::select(from, lon, lat) %&gt;% \n      mutate(across(.cols = c(lon, lat), as.character)) , by = c(\"lon\", \"lat\"), \n    multiple = \"all\"\n  ) %&gt;%  \n  left_join(\n    rl_sum_sf %&gt;% \n      mutate(\n        lon = st_coordinates(.)[,\"X\"]\n      ) %&gt;% \n      st_drop_geometry() %&gt;% \n      rename(to = rec_name,\n             llon = lon) %&gt;% # join for the tos  \n      dplyr::select(to, llon) %&gt;% \n      mutate(llon = as.character(llon)), by = c(\"llon\"), \n    multiple = \"all\"\n  ) %&gt;% \n  mutate(\n    from_to = paste0(from, \"-\", to), \n    id = 1:nrow(.)\n  ) %&gt;% \n  dplyr::select(id, from, to, from_to, lon, lat, llon, llat) %&gt;% \n  mutate(across(.col = c(lon, lat, llon, llat), as.numeric))\n\nAwesome! We have all of our combinations with their names and we now know which paths go from one receiver to another. The only issue is all of points are in decimal degrees with a CRS of WGS 84, we need to convert this into to UTMs.\nBe sure to choose the correct UTM zone here. This vignette uses UTM zone 15 north but for other uses you will have to change the UTM zone.\n\nrec_order_utm &lt;- st_as_sf(rec_order, \n                          coords = c(\"lon\", \"lat\"), \n                          crs = st_crs(rl_sum_sf)) %&gt;% \n  st_transform(crs = 32615) %&gt;% \n  mutate(\n    lon = st_coordinates(.)[,\"X\"], # grab lon \n    lat = st_coordinates(.)[,\"Y\"]  # grab lat \n  ) %&gt;% \n  st_drop_geometry() %&gt;% \n  st_as_sf(., coords = c(\"llon\", \"llat\"), \n           crs = st_crs(rl_sum_sf)) %&gt;% \n  st_transform(crs = 32615) %&gt;% \n  mutate(\n    llon = st_coordinates(.)[,\"X\"], # grab lon \n    llat = st_coordinates(.)[,\"Y\"]  # grab lat \n  ) %&gt;% \n  st_drop_geometry()\n\n\n\n0.6 Make shortest paths\nWe will first split our combinations into individual end points, then use purrr::map() to iterate over each combination and use the shortestPath() function to calculate the shortest path for every combination.\nWe then will transform the output of this which are SpatialLinesDataFrame to sf objects. Important note here is to change the CRS to your specific CRS UTM zone.\n\nrec_dist_sf &lt;- rec_order_utm %&gt;%\n  split(.$id) %&gt;%\n  map(possibly(~ shortestPath(trans,\n                              c(.$llon, .$llat),\n                              c(.$lon, .$lat),\n                              output = \"SpatialLines\"), NA)) %&gt;%\n  map(possibly(~ st_as_sf(., crs = 32615), NA)) %&gt;% # u will need to replace CRS\n  bind_rows(.id = \"id\") %&gt;%\n  mutate(\n    cost_dist = as.numeric(st_length(.))\n  )\n\n\n\n0.7 Add in metadata of paths start and end desitantions\nFirst we will change the id column to a character to be able to line up the data properly.\n\nrec_order_names &lt;- rec_order_utm %&gt;% \n  mutate(\n    id = as.character(id)\n  )\n\nNext we will use left_join() from {dplyr} to connect each path’s metadata.\n\nrec_dist_sf &lt;- rec_dist_sf %&gt;% \n  left_join(rec_order_names, by = \"id\") %&gt;% \n  dplyr::select(id, from:llon, cost_dist, geometry)\n\n\n\n0.8 Plot\nWe will use ggplot to look at our paths. Lets first check if paths go to the right locations and then we will plot the whole thing.\n\nggplot() +\n  geom_sf(data = lake_utm) +\n  geom_sf(data = rec_dist_sf %&gt;% \n            filter(from_to %in% \"3-12\")\n          , aes(colour = cost_dist), size = 1) +\n  geom_sf_label(data = rl_sum_utm , size = 4, aes(label = rec_name)) +\n  scale_colour_viridis_c(name = \"Cost Distance (m)\", option = \"B\") +\n  theme_void()\n\n\n\nggplot() +\n  geom_sf(data = lake_utm) +\n  geom_sf(data = rec_dist_sf, aes(colour = cost_dist), size = 1) +\n  geom_sf(data = rl_sum_utm , size = 4) +\n  scale_colour_viridis_c(name = \"Cost Distance (m)\", option = \"B\") +\n  theme_void()\n\n From here the sf object can be kept together or ripped apart to determine the distance or path a fish could swim within the system along with a whole host of other potential implications (e.g. interpolated paths).\nCredit: R. Lennox, PhD, Incoming Science Director - OTN for the original ideas around this script."
  },
  {
    "objectID": "posts/post-with-code/nichetools-siber/index.html",
    "href": "posts/post-with-code/nichetools-siber/index.html",
    "title": "How to use {nichetools} with SIBER",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to use {SIBER} and {nichetools} to extract and then visualize estimates of trophic niche size and similarities and Layman community metrics for multiple freshwater fish using {ggplot2}.\nThis vignette can be used for additional purposes including estimating niche size and similarities among different groups of aquatic and/or terrestrial species. Furthermore, niche size and similarities for different behaviours exhibited within a population can be made using behavioural data generated from acoustic telemetry (e.g., differences in habitat occupancy).\n\n\n0.2 Bring in trophic data\nFirst we will load the necessary packages to preform the analysis and visualization. We will use {SIBER} and {nichetools} to preform the analysis. We will use {bayestestR} to calculate and extract medians and Equal-Tailed Interval (ETI) of posterior distributions that we want to plot. We will use {dplyr}, {tidyr}, and {purrr} to manipulate data and iterate processes. Lastly, we will use {ggplot2}, {ggtext}, and {ggdist} to plot and add labels.\nI will add that many of the {dplyr} and {tidyr} functions and processes can be replaced using {data.table} which is great when working with large data sets.\nWe will first load the packages that were just mentioned\n\n{\n  library(bayestestR)\n  library(dplyr)\n  library(ggplot2)\n  library(ggdist)\n  library(ggtext)\n  library(nichetools)\n  library(purrr)\n  library(SIBER)\n  library(tidyr)\n  library(viridis)\n}\n\nWarning: package 'nichetools' was built under R version 4.4.1\n\n\nFor the purpose of the vignette we will be using the demo.siber.data.2 data frame that is available within {SIBER}. We will first look at the structure of this data frame using the {dplyr} function glimpse(). For your purposes you will need to replace this with your data frame either by loading a csv, rds, or qs file. You can do this multiple ways, I prefer using readr::read_csv() for a csv but base R’s read.csv() works perfectly fine. Note, {SIBER} functions do not take tibbles or data.tables so you will have to convert either to data.frame class prior to running functions in {SIBER}.\n\nglimpse(demo.siber.data.2)\n\nRows: 150\nColumns: 4\n$ iso1      &lt;dbl&gt; -11.048902, -8.809360, -9.256212, -8.380952, -10.401561, -7.…\n$ iso2      &lt;dbl&gt; 1.02044315, 1.97597511, 3.28707260, -1.95750266, -3.11949113…\n$ group     &lt;chr&gt; \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"cit…\n$ community &lt;chr&gt; \"dublin\", \"dublin\", \"dublin\", \"dublin\", \"dublin\", \"dublin\", …\n\n\nYou will notice that the community and group column are character strings that are the actual names of the communities and groups. I advise changing them into factors, thus allow you to know the order for each column prior to converting them into a numeric and then a charcter. The reason why this is important, is that functions in {SIBER} use for loops based on the indexing of the SiberObject. If this order does not match up you can have issues with the names of the communities and groups you are working with.\nLet us change these into a factor, then preserve the column and create an id column that is the numerical order that will become the community and group names provided to createSiberObject().\n\ndemo.siber.data.2 &lt;- demo.siber.data.2 %&gt;%\n  mutate(\n    group = factor(group), \n    community = factor(community), \n    group_id = as.numeric(group) %&gt;% \n      as.character(),\n    community_id = as.numeric(community) %&gt;%\n      as.character()\n  ) %&gt;% \n  rename(\n    group_name = group,\n    community_name = community,\n    group = group_id,\n    community = community_id\n  )\n\nglimpse(demo.siber.data.2)\n\nRows: 150\nColumns: 6\n$ iso1           &lt;dbl&gt; -11.048902, -8.809360, -9.256212, -8.380952, -10.401561…\n$ iso2           &lt;dbl&gt; 1.02044315, 1.97597511, 3.28707260, -1.95750266, -3.119…\n$ group_name     &lt;fct&gt; city, city, city, city, city, city, city, city, city, c…\n$ community_name &lt;fct&gt; dublin, dublin, dublin, dublin, dublin, dublin, dublin,…\n$ group          &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ community      &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nAfter we have done this, we are going to create two data frames that are the names of our communities and groups with their associated id values. We will use these data frames later on to join up the actual names, allowing us to know what estimates belong to which communities and groups. We are doing this because it is unlikely you will have communities and groups named 1, 2, 3 ect. and instead will have actual names.\n\n# ---- create name with group and community data frame ----\ncg_names &lt;- demo.siber.data.2 %&gt;%\n  distinct(group,\n           community, \n           group_name, \n           community_name) %&gt;%\n  \n  arrange(community, group)\n\n# ---- create community names data frame ---- \nc_names &lt;- demo.siber.data.2 %&gt;% \n  distinct(community, \n           community_name) %&gt;%\n  arrange(community)\n\nWe will then plot our biplot to confirm we have the correct structure.\n\nggplot(data = demo.siber.data.2, aes(x = iso1, y = iso2,\n                                     colour = group_name)) +\n  geom_point() +\n  facet_wrap(~ community_name) + \n  scale_colour_viridis_d(option = \"A\", begin = 0.25, end = 0.85,\n                         name = \"Groups\", alpha = 0.75) + \n  theme_bw(\n    base_size = 15\n  ) + \n  theme(\n    strip.background = element_blank(),\n    panel.grid = element_blank(), \n    axis.title = element_markdown(),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.65, 0.75)\n  ) + \n  labs(\n    x = paste0(\"\\U03B4\",\"&lt;sup&gt;\", 13, \"&lt;/sup&gt;\", \"C\", \" (‰)\"),\n    y = paste0(\"\\U03B4\",\"&lt;sup&gt;\", 15, \"&lt;/sup&gt;\", \"N\", \" (‰)\")\n  )\n\n\n\n\n\n\n\n\nNext we will grab the isotopes we need and the community and group ids that have already been renamed to community and group. This is important as creatSiberObject() will 1) only take the following order with the following names iso1, iso2, group, and community and 2) we will transform this tibble into a data.frame as {SIBER} will only work with a data.frame. In this case we were using tibbles but we could also be working with data.table and will need to do the same thing.\n\ndemo_siber_data &lt;- demo.siber.data.2 %&gt;% \n  dplyr::select(iso1, iso2, group, community) %&gt;%\n  arrange(community, group) %&gt;% \n  as.data.frame() \n\n\n\n0.3 Convert to {SIBER} object\nFirst convert to our isotope data into a {SIBER} object.\n\nsiber_example &lt;- createSiberObject(demo_siber_data)\n\nNow that this is a {SIBER} object we can start doing some analysis using a frequentist (e.g., maximum-likelihood) or a Bayesian framework. The data and metrics generated through this analysis by {SIBER} can be extracted using functions in {nichetools}.\n\n\n0.4 Bayesian Ellipse Analysis\nWe first need to set the parameters to run the model\n\n# options for running jags\nparms &lt;- list()\nparms$n.iter &lt;- 2 * 10^4   # number of iterations to run the model for\nparms$n.burnin &lt;- 1 * 10^3 # discard the first set of values\nparms$n.thin &lt;- 10     # thin the posterior by this many\nparms$n.chains &lt;- 2        # run this many chains\n\nNext we need to define the priors for each parameter of the model. This includes fitting the ellipses using an Inverse Wishart prior on the covariance matrix (\\(\\Sigma\\)), and a vague normal prior on the means (\\(\\mu\\)).\n\n# fit the ellipses which uses an Inverse Wishart prior on the \n# covariance matrix Sigma, and a vague normal prior on the \n# means.\npriors &lt;- list()\npriors$R &lt;- 1 * diag(2)\npriors$k &lt;- 2\npriors$tau.mu &lt;- 1.0E-3\n\nWe will now run the model using the function siberMVN().\n\nellipses_posterior &lt;- siberMVN(siber_example, parms, priors)\n\n\n\n0.5 Extract posterior distributions for μ and Σ\nWe will first extract posterior distribution for \\(\\mu\\) using extract_mu() in {nichetools}. We will need to set the argument pkg to \"SIBER\" and the argument data_format to \"wide\". This argument takes \"long\" or \"wide\" which dictates whether the data object returned is in wide or long format. We will also use the function seperate_wider_delim() from {tidyr} to separate the community and groups names as they are joined by a .. We then will use left_join() and the cg_names data frame we created above to add in the correct community and group names.\n\ndf_mu &lt;- extract_mu(ellipses_posterior, pkg = \"SIBER\", \n                    data_format = \"wide\") %&gt;%\n  separate_wider_delim(sample_name, cols_remove = FALSE,\n                       delim = \".\", names = c(\"community\",\n                                              \"group\")) %&gt;%\n  left_join(cg_names)\n\nWe can confirm that the posterior estimates of \\(\\mu\\) are correct by plotting them with {ggplot2}.\n\nggplot() +\n  geom_point(data = df_mu, aes(x = d13c, y = d15n,\n                               colour = group_name)) +\n  geom_point(data = demo.siber.data.2, aes(x = iso1, y = iso2,\n                                           colour = group_name)) +\n  facet_wrap( ~ community_name) + \n  scale_colour_viridis_d(option = \"A\", begin = 0.25, end = 0.85,\n                         name = \"Groups\", alpha = 0.75) + \n  theme_bw(\n    base_size = 15\n  ) + \n  theme(\n    strip.background = element_blank(),\n    panel.grid = element_blank(), \n    axis.title = element_markdown(),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.65, 0.75)\n  ) + \n  labs(\n    x = paste0(\"\\U03B4\",\"&lt;sup&gt;\", 13, \"&lt;/sup&gt;\", \"C\", \" (‰)\"),\n    y = paste0(\"\\U03B4\",\"&lt;sup&gt;\", 15, \"&lt;/sup&gt;\", \"N\", \" (‰)\")\n  )\n\n\n\n\n\n\n\n\nNotice the density of points in the center of the raw data for the corresponding colours. This density of points are the posterior estimates from the model for \\(\\mu\\) and are an indication that the model is iterating over the groups and communities correctly.\nWe are also going to extract \\(\\mu\\) in long format for creating ellipse, as the function that create ellipse needs the data frame of \\(\\mu\\) to be in long format.\n\ndf_mu_long &lt;- extract_mu(ellipses_posterior, pkg = \"SIBER\") %&gt;% \n  separate_wider_delim(sample_name, cols_remove = FALSE,\n                       delim = \".\", names = c(\"community\",\n                                              \"group\")) %&gt;%\n  left_join(cg_names)\n\nNext we are going to extract posterior estimates of \\(\\Sigma\\) using extract_sigma().\n\ndf_sigma &lt;- extract_sigma(ellipses_posterior, pkg = \"SIBER\")\n\nLastly, we are going to feed each \\(\\mu\\) and \\(\\Sigma\\) estimate to niche_ellipse() to estimate each ellipse. There are few things to know about this function and they include the following: 1) it will randomly sample 10 ellipse from the total posterior distribution of \\(\\mu\\) and \\(\\Sigma\\), this seems quite standard, however, you can adjust the number of samples by changing the argument n. 2) to make the function consistently randomly sample the same set you will need to set set_seed to a numerical value. If this is not set then it will randomly sample a different set of 10 ellipses every time. 3) if you would like the function to not randomly sample set the argument random to FALSE. 4) by default it will tell you how long it takes to generate the ellipse and will have progress bars at each step. If you want to turn this off set message to FALSE. 5) if you are wanting to change the confidence level of the ellipse you can do so using the argument p_ell. This value is bound between 0 and 1.\n\ndf_el &lt;- niche_ellipse(dat_mu = df_mu_long,\n                       dat_sigma = df_sigma, \n                       set_seed = 4, n = 20) %&gt;%\n  separate_wider_delim(sample_name, cols_remove = FALSE,\n                       delim = \".\", names = c(\"community\",\n                                              \"group\")) %&gt;%\n  left_join(cg_names)\n\nNow that we have the ellipses created we can plot them.\n\nggplot(data = df_el, \n       aes(x = d13c, y = d15n, \n           group = interaction(sample_number, \n                               sample_name), \n           colour = group_name)) + \n  geom_polygon(linewidth = 0.5, fill = NA) + \n  facet_wrap( ~ community_name) + \n  scale_colour_viridis_d(option = \"A\", begin = 0.25, end = 0.85,\n                         name = \"Groups\", alpha = 0.75) + \n  theme_bw(\n    base_size = 15\n  ) + \n  theme(\n    strip.background = element_blank(),\n    panel.grid = element_blank(), \n    legend.position = \"inside\",\n    axis.title = element_markdown(),\n    legend.background = element_blank(), \n    legend.position.inside = c(0.65, 0.75)\n  ) + \n  labs(\n    x = paste0(\"\\U03B4\",\"&lt;sup&gt;\", 13, \"&lt;/sup&gt;\", \"C\", \" (‰)\"),\n    y = paste0(\"\\U03B4\",\"&lt;sup&gt;\", 15, \"&lt;/sup&gt;\", \"N\", \" (‰)\")\n  )\n\n\n\n\n\n\n\n\n\n\n0.6 Extract Niche Size\nWe can use siberEllipses() from {SIBER} to estimate niche size for each posterior sample for each community and group.\n\nsea_b &lt;- siberEllipses(corrected.posteriors = ellipses_posterior)\n\nNext using extract_niche_size() from {nichetools} we can extract niche size which will also add in the correct names for the communities and groups we are working with.\n\nseb_convert &lt;- extract_niche_size(data = sea_b,\n                                  pkg = \"SIBER\",\n                                  community_df = cg_names)\n\nWe will also extract the parametric estimate for niche size using groupMetricsML() from {SIBER}.\n\ngroup_ml &lt;- groupMetricsML(siber_example)\n\nWe can convert the output of this function using extract_group_metrics().\n\ngroup_convert &lt;- extract_group_metrics(data = group_ml,\n                                       community_df = cg_names)\n\nThe object returned will have maximum-likelihood estimates for the standard ellipse area (SEA), the central standard ellipse area (SEAc), and the total area (TA). For plotting niche size we will use SEAc.\n\nsea_c &lt;- group_convert %&gt;% \n  filter(metric == \"SEAc\")\n\nLastly, we can visualize the extracted niche sizes using {ggplot2}.\n\nggplot() +\n  geom_violin(data = seb_convert, aes(x = community_name,\n                                      y = sea,\n                                      fill = group_name)) + \n  scale_fill_viridis_d(option = \"A\", begin = 0.25, end = 0.85,\n                       name = \"Groups\", alpha = 0.75) + \n  geom_point(data = sea_c, aes(x = community_name, \n                               y = est, \n                               group = group_name,\n  ), \n  size = 2.5,\n  fill = \"white\",\n  shape = 21,\n  position = position_dodge(width = 0.9)) +\n  theme_bw(\n    base_size = 15\n  ) + \n  theme(\n    strip.background = element_blank(),\n    panel.grid = element_blank(), \n    legend.position = \"inside\",\n    legend.background = element_blank(), \n    legend.position.inside = c(0.85, 0.8)\n  ) + \n  labs(\n    x = \"Communities\", \n    y = expression(paste(\"Niche Size p(\", \"‰\"^2, \"| x)\"))\n  )\n\n\n\n\n\n\n\n\n\n\n0.7 Niche Similarties\nNow that we have extracted Bayesian estimates of niche size we are likely wanting to know how much do these niches have in common or are similar.\nWe can use maximum-likelihood and Bayesian frameworks to estimate the percentage of similarity within communities between groups and/or among communities with groups being consistent. We can use functions in {SIBER} to create maximum-likelihood and Bayesian estimates of niche similarity followed by functions in {nichetools} to extract these similarities.\nFirst we need to create our comparisons that we are wanting to evaluate. We can use the function create_comparisons() to generate a list that has two-column data frames of each comparison. You can change the argument comparison to \"among\" to compare communities for the same group, versus \"within\" which compares groups within a community.\n\ncg_names_within_com &lt;- cg_names %&gt;%\n  create_comparisons(comparison = \"within\")\n\nNext we can feed this listed data frames to either maxLikOverlap() or bayesianOverlap() using map() from {purrr}. For this exercise I have not changed .progress argument of map() but when working with large data sets I often turn this argument to TRUE to provide a progress update.\n\nml_within_overlap &lt;- cg_names_within_com %&gt;%\n  map(~ maxLikOverlap(.x$cg_1, .x$cg_2, siber_example,\n                      p.interval = 0.95, n = 100))\n\nNext we can extract estimates using extract_similarities() with the type argument set to \"ml\".\n\nml_95_within_com &lt;- extract_similarities(ml_within_overlap, \n                                         type = \"ml\", \n                                         community_df = cg_names)\n\nNow we will repeat the process for bayesianOverlap(), first supplying the function with our list of data frames, next having map() iterate over this list.\n\nbayes95_overlap &lt;- cg_names_within_com %&gt;%\n  map(~ bayesianOverlap(.x$cg_1, .x$cg_2, ellipses_posterior,\n                        draws = 100, p.interval = 0.95,\n                        n = 100)\n  )\n\nNext we can extract estimates using extract_similarities() with the type argument set to \"bay\".\n\nbays_95_overlap &lt;- extract_similarities(bayes95_overlap, \n                                        type = \"bay\",\n                                        community_df = cg_names)\n\nNow that we have extracted maximum-likelihood and Bayesian estimates we can visualize them.\nPrior to creating a point interval plot, we need to create a colour palette that will be used to identify each community.\n\nviridis_colors_s &lt;- viridis(4, begin = 0.25, end = 0.85,\n                          option = \"A\",\n                          alpha = 0.75\n)\n\nNow we can use point interval plots to visually represent posterior distributions.\n\nggplot() + \n  stat_pointinterval(data = bays_95_overlap,\n              aes(x = group_1, \n                  y = prop_overlap, \n                  point_fill = group_2), \n    interval_colour = \"grey60\",\n    point_size = 3,\n    shape = 21,\n    position = position_dodge(0.4)) + \n  \n  geom_point(data = ml_95_within_com, aes(x = group_1, \n                                   y = prop_overlap,\n                                   group = group_2), \n             shape = 21,\n             fill = \"white\",\n             size = 2,\n             alpha = 0.5, \n             position = position_dodge(0.4)) + \n  scale_fill_manual(name = \"Group\", \n                    aesthetics = \"point_fill\",\n                    values = viridis_colors_s) + \n  theme_bw(\n    base_size = 15\n  ) + \n  theme(\n    panel.grid = element_blank(),\n    strip.background = element_blank(),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.15, 0.80),\n  ) +\n  labs(\n    x = \"Group\",\n    y = expression(paste(\"p(\", \"‰\", \"|X)\"))\n  )\n\n\n\n\n\n\n\n\n\n\n0.8 Bayesian Estimates of Layman’s Community Metrics\nFirst I highly recommend reading Layman et al. 2007 to understand the six community metrics that will be estimating using a Bayesian framework in this section.\nTo create maximum-likelihood estimate of Layman’s community metrics we first need to use communityMetricsML() from {SIBER}.\n\ncommunity_ml &lt;- communityMetricsML(siber_example)\n\nThen we can extract these estimates using extract_layman() with the type argument set to \"ml\". The default for this argument is \"bay\".\n\nlayman_ml &lt;- extract_layman(community_ml, \n                            type = \"ml\", \n                            community_df = c_names)\n\nTo create Bayesian estimates of Layman’s community metrics we first need to use extractPosteriorMeans() from {SIBER} to extract posterior estimates of means for each community and group.\n\nmu_post &lt;- extractPosteriorMeans(siber_example, ellipses_posterior)\n\nNext we need to use bayesianLayman() from {SIBER} and use our extracted posterior means to create Bayesian estimates for each community metric.\n\nlayman_b &lt;- bayesianLayman(mu.post = mu_post)\n\nOnce we have created our Bayesian estimates for each community metric we can use extract_layman() to extract these estimates. The function will also create the variable labels which will first assign a new name to each community metric and secondly will reorder these names based on how these metrics are often viewed.\n\nlayman_be &lt;- extract_layman(layman_b, community_df = c_names)\n\nPrior to creating a point interval plot, we need to create a colour palette that will be used to identify each community.\n\nviridis_colors &lt;- viridis(2, begin = 0.25, end = 0.85,\n                          option = \"G\",\n                          alpha = 0.75\n)\n\nLastly, we can visualize the distributions of the posterior estimates using stat_pointinterval() from {ggdist}.\n\nggplot() + \n  stat_pointinterval(\n    data = layman_be, aes(x = labels, \n                          y = post_est, \n                          point_fill = community_name),\n    point_size = 2.5,\n    interval_colour = \"grey60\",\n    position = position_dodge(0.4),\n    shape = 21\n  ) + \n  geom_point(data = layman_ml, aes(x = labels, \n                                   y = estimate,\n                                   group = community_name), \n             shape = 21,\n             fill = \"white\",\n             alpha = 0.5, \n             position = position_dodge(0.4)) + \n  scale_fill_manual(name = \"Community\", \n                    aesthetics = \"point_fill\",\n                    values = viridis_colors) + \n  theme_bw(\n    base_size = 15\n  ) + \n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_markdown(),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.88, 0.85),\n  ) +\n  labs(\n    x = \"Community Metrics\",\n    y = expression(paste(\"p(\", \"‰\", \"|X)\"))\n  )\n\n\n\n\n\n\n\n\nCongratulations, we have successfully used functions from {SIBER} to extract and visually represent trophic communities and niche size and similarities. If you something doesn’t work/is confusing please reach out."
  },
  {
    "objectID": "posts/post-with-code/nicheROVER-ggplot/nicheROVER_ggplot.html",
    "href": "posts/post-with-code/nicheROVER-ggplot/nicheROVER_ggplot.html",
    "title": "Estimating Trophic Niches - {nicheROVER} and {ggplot2}",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to use {ggplot2} to visualize estimates of trophic niche size and similarities for multiple freshwater fish.\nThis vignette can be used for additional purposes including estimating niche size and similarities among different groups of aquatic and/or terrestrial species. Furthermore, niche size and similarities for different behaviours exhibited within a population can be made using behavioural data generated from acoustic telemetry (e.g., differences in habitat occupancy).\nYou can download and unzip this vignette using the following code:\n\ninstall.packages(\"usethis\")\nusethis::use_course(\"https://github.com/benjaminhlina/nicheROVER-ggplot-vignette/archive/refs/heads/main.zip\")\n\n\n\n0.2 Disclaimer\nSections of this vignette have been replaced with the package {nichetools} which improves this workflow. I have written a vignette for the package that is available on the package’s website or on this blog. I highly suggest using that workflow going forward. This vignette will not be taken down but will also not be maintained.\n\n\n0.3 Bring in trophic niche data\nFirst we will load the necessary packages to preform the analysis and visualization. We will use {nicheROVER} and {ellipse} to preform the analysis. We will use {dplyr}, {purrr}, and {tidyr} to manipulate data and iterate processes. Lastly, we will use {ggplot2}, {ggtext}, and {patchwork} to plot, add labels and arrange plots.\nI will add that many of the {dplyr} and {tidyr} functions and processes can be replaced using {data.table} which is great when working with large datasets.\n\n{\n  library(dplyr)\n  library(ellipse)\n  library(ggplot2)\n  library(ggtext)\n  library(here)\n  library(nicheROVER) \n  library(purrr)\n  library(patchwork)\n  library(readr)\n  library(stringr)\n  library(tidyr)\n}\n\nFor the purpose of the vignette we will be using the fish dataframe that is available within {nicheROVER}. We will remove \\(\\delta\\)34S for simplicity of the vignette. If more than two isotopes or metrics are being used to compare niche sizes and similarities, you can modify the code to include the additional isotopes or metrics.\nWe will first use the function janitor::clean_names() to clean up column names and remove \\(\\delta\\)34S column. For your purposes you will need to replace fish with your dataframe either by loading a csv or rds with your data. You can do this multiple ways, I prefer using readr::read_csv() but base R works perfectly fine.\n\ndf &lt;- fish %&gt;% \n  janitor::clean_names() %&gt;% \n  select(-d34s)\n\nIf there are any isotopic values that did not run and are NA, they will need to be removed because {nicheROVER}’s functions will not accommodate values of NA.\n\n\n0.4 Estimate posterior distribution with Normal-Inverse-Wishart (NIW) priors.\nWe will take 1,000 posterior samples for each group. You can change this but suggest nothing less than 1,000.\n\nnsample &lt;- 1000\n\nWe will then split the dataframe into a list with each species as a dataframe object within the list, We will then iterate over the list, using map() from {purrr}, to estimate posterior distribution using Normal-Inverse-Wishart (NIW) priors.\n\nfish_par &lt;- df %&gt;% \n  split(.$species) %&gt;% \n  map(~ select(., d13c, d15n)) %&gt;% \n  map(~niw.post(nsample = nsample, X = .))\n\n\n\n0.5 Extract \\(\\mu\\) values\nWe will use a combination of map() and pluck() to first extract the list of posteriors for \\(\\mu\\). We will extract each vector object for \\(\\mu\\) of each species using imap() and convert them into a tibble.\nWe then will merge each \\(\\mu\\) dataframe together for each species using bind_rows(). We will add species and sample_number back into the dataframe.\n\ndf_mu &lt;- map(fish_par, pluck, 1) %&gt;% \n  imap(~ as_tibble(.x) %&gt;% \n         mutate( \n           metric = \"mu\", \n           species = .y\n         )\n  ) %&gt;%\n  bind_rows() %&gt;% \n  mutate(\n    species = factor(species, \n                     levels = c(\"ARCS\", \"BDWF\", \"LKWF\", \"LSCS\"))\n  ) %&gt;% \n  group_by(species) %&gt;% \n  mutate(\n    sample_number = 1:1000\n  ) %&gt;% \n  ungroup()\n\nWe need to manipulate df_mu into long format instead of wide format for the rest of the analysis. We will also add in a column that is the element abbreviation and neutron number to be used in axis labelling.\n\ndf_mu_long &lt;- df_mu %&gt;% \n  pivot_longer(cols = -c(metric, species, sample_number), \n               names_to = \"isotope\", \n               values_to = \"mu_est\") %&gt;% \n  mutate(\n    element = case_when(\n      isotope == \"d15n\" ~ \"N\",\n      isotope == \"d13c\" ~ \"C\",\n    ), \n    neutron = case_when(\n      isotope == \"d15n\" ~ 15,\n      isotope == \"d13c\" ~ 13,\n    ) \n  )\n\n\n\n0.6 Extract \\(\\Sigma\\) values\nWe will use a combination of map() and pluck() to first extract the list of posteriors for \\(\\Sigma\\). We will extract each vector object from each of the \\(\\Sigma\\) vectors for each species using imap() and convert them into a tibble.\nWe will manipulate df_sigma from wide to long format. When doing so we create two columns, id and isotope, that identify the two isotopes that \\(\\Sigma\\) is being estimated for.\n\ndf_sigma &lt;- map(fish_par, pluck, 2) %&gt;%\n  imap(~ as_tibble(.x) %&gt;%\n         mutate(\n           metric = \"sigma\",\n           id = c(\"d15n\", \"d13c\"),\n           species = .y\n         )\n  ) %&gt;%\n  bind_rows() %&gt;%\n  pivot_longer(cols = -c(\"id\", \"species\", \"metric\"),\n               names_to = \"isotope\",\n               values_to = \"post_sample\"\n  )  %&gt;%\n  separate(isotope, into = c(\"isotopes\", \"sample_number\"), sep = \"\\\\.\")\n\nWe then need to remove \\(\\Sigma\\) values for when the two columns are the same isotope.\n\ndf_sigma_cn &lt;- df_sigma %&gt;%\n  filter(id != isotopes)\n\n\n\n0.7 Plot posterior distrubtion of \\(\\mu\\) and \\(\\Sigma\\)\nFor most plotting within this vignette, I will split() the dataframe by isotope, creating a list that I will then use imap() to iterate over the list to create plots. We will use geom_density() to represent densities for both \\(\\mu\\) and \\(\\Sigma\\). Plot objects will then be stored in a list.\nFirst we will plot \\(\\mu\\) for each isotope. We will use {patchwork} to configure plots for multi-panel figures. This package is phenomenal and uses math operators to configure and manipulate the plots to create multi-panel figures.\nFor labelling we are also going to use element_markdown() from {ggtext} to work with the labels that are needed to correctly display the isotopic signature. If you are working other data please replace.\n\nposterior_plots &lt;- df_mu_long %&gt;%\n  split(.$isotope) %&gt;%\n  imap(\n    ~ ggplot(data = ., aes(x = mu_est)) +\n      geom_density(aes(fill = species), alpha = 0.5) +\n      scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                           option = \"D\", name = \"Species\") +\n      theme_bw() +\n      theme(panel.grid = element_blank(),\n            axis.title.x =  element_markdown(),\n            axis.title.y =  element_markdown(),\n            legend.position = \"none\"\n      ) +\n      labs(\n        x = paste(\"\\u00b5&lt;sub&gt;\\U03B4&lt;/sub&gt;\", \"&lt;sub&gt;&lt;sup&gt;\",\n                  unique(.$neutron), \"&lt;/sup&gt;&lt;/sub&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element), \"&lt;/sub&gt;\", sep = \"\"),\n        y = paste0(\"p(\\u00b5 &lt;sub&gt;\\U03B4&lt;/sub&gt;\",\"&lt;sub&gt;&lt;sup&gt;\",\n                   unique(.$neutron), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                   \"&lt;sub&gt;\",unique(.$element),\"&lt;/sub&gt;\",\n                   \" | X)\"), sep = \"\")\n  )\n\nposterior_plots$d15n +\n  theme(legend.position = c(0.18, 0.84)) + \n  posterior_plots$d13c\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nFor labeling purposes we need to add columns that are the element abbreviation and neutron number. I do this by using case_when() which are vectorized if else statements.\n\ndf_sigma_cn &lt;- df_sigma_cn %&gt;%\n  mutate(\n    element_id = case_when(\n      id == \"d15n\" ~ \"N\",\n      id == \"d13c\" ~ \"C\",\n    ),\n    neutron_id = case_when(\n      id == \"d15n\" ~ 15,\n      id == \"d13c\" ~ 13,\n    ),\n    element_iso = case_when(\n      isotopes == \"d15n\" ~ \"N\",\n      isotopes == \"d13c\" ~ \"C\",\n    ),\n    neutron_iso = case_when(\n      isotopes == \"d15n\" ~ 15,\n      isotopes == \"d13c\" ~ 13,\n    )\n  )\n\nNext we will plot the posteriors for \\(\\Sigma\\).\n\nsigma_plots &lt;- df_sigma_cn %&gt;%\n  group_split(id, isotopes) %&gt;%\n  imap(\n    ~ ggplot(data = ., aes(x = post_sample)) +\n      geom_density(aes(fill = species), alpha = 0.5) +\n      scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                           option = \"D\", name = \"Species\") +\n      theme_bw() +\n      theme(panel.grid = element_blank(),\n            axis.title.x =  element_markdown(),\n            axis.title.y =  element_markdown(),\n            legend.position = \"none\"\n      ) +\n      labs(\n        x = paste(\"\\U03A3\",\"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_id), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_id),\"&lt;/sub&gt;\",\" \",\n                  \"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_iso), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_iso),\"&lt;/sub&gt;\", sep = \"\"),\n        y = paste(\"p(\", \"\\U03A3\",\"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_id), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_id),\"&lt;/sub&gt;\",\" \",\n                  \"&lt;sub&gt;\\U03B4&lt;/sub&gt;\",\n                  \"&lt;sub&gt;&lt;sup&gt;\", unique(.$neutron_iso), \"&lt;/sub&gt;&lt;/sup&gt;\",\n                  \"&lt;sub&gt;\",unique(.$element_iso),\"&lt;/sub&gt;\", \" | X)\", sep = \"\"),\n      )\n  )\n\nsigma_plots[[1]] + \n  theme(legend.position = c(0.1, 0.82))\n\n\n\n\n\n\n\n\n\n\n0.8 Estimate niche ellipse\nWe need to manipulate df_sigma back to wide format for ellipses.\n\ndf_sigma_wide &lt;- df_sigma %&gt;%\n  pivot_wider(names_from = id,\n              values_from = post_sample)\n\nNext we will use a for loop to estimate niche ellipses for all 1,000 samples. The default confidence level is 0.95 but can be adjusted by changing p.ell.\n\np.ell &lt;- 0.95\n\nWe will then create a vector of unique species (i.e., groups), that we loop over.\n\nspecies_name &lt;- unique(df_sigma_wide$species)\n\nNext create an empty list to dump the results of the for loop.\n\nall_ellipses &lt;- list()\n\nWe then will use the following for loop. First it subsets mu and sigma objects by species. Then it extracts \\(\\mu\\) and \\(\\Sigma\\) values for each sample number for each isotope and each species. Then \\(\\mu\\) and \\(\\Sigma\\) are given to ellipse() from {ellipse} that will generate a unique ellipse with a confidence interval of p.ell for each sample (e.g., 1,000 samples). We need to add in dummy variables (ell and post.id) in the first loop that we will add to within the second loop.\nIf you are to have additional isotopes or metrics, you will need to modify this loop to include them. Specifically ellipse() can only work within two-dimensions, not three, so you will have to create multiple ellipse() calls for each combination of isotopes or metrics.\n\nfor (i in 1:length(species_name)) {\n  \n  sigma_species &lt;- df_sigma_wide %&gt;% \n    filter(species %in% species_name[i])\n  \n  mu_species &lt;- df_mu %&gt;% \n    filter(species %in% species_name[i])\n  \n  ell &lt;- NULL\n  post.id &lt;- NULL\n  \n  for(j in 1:length(unique(sigma_species$sample_number))) {\n    \n    sigma_ind &lt;- sigma_species %&gt;%\n      filter(sample_number %in% sample_number[j]) %&gt;% \n      dplyr::select(d15n, d13c) \n    \n    Sigma &lt;- as.matrix(sigma_ind, 2, 2)\n    row.names(Sigma) &lt;- c(\"d15n\", \"d13c\")\n    \n    mu &lt;- mu_species %&gt;%\n      filter(sample_number %in% sample_number[j]) %&gt;% \n      dplyr::select(sample_number, d15n, d13c) %&gt;% \n      pivot_longer(cols = -sample_number, \n                   names_to = \"isotope\", \n                   values_to = \"mu\") %&gt;% \n      .$mu\n    \n    out &lt;- ellipse::ellipse(Sigma, centre = mu, which = c(1, 2), level = p.ell)\n    \n    ell &lt;- rbind(ell, out)\n    post.id &lt;- c(post.id, rep(j, nrow(out)))\n  }\n  ell &lt;- as.data.frame(ell)\n  ell$rep &lt;- post.id\n  all_ellipses[[i]] &lt;- ell\n}\n\nWe then will then take the resulting list and merge together to create a dataframe that can be used in plotting. I use the argument .id to designate an unique number to each object in the list because each dataframe object in the list is an estimated ellipse for each species (e.g., 1-4). we then will use case_when() to add in a column with our species abbreviations.\n\n# combine ellipose list into dataframe and add species names back in \nellipse_df &lt;- bind_rows(all_ellipses, .id = \"id\") %&gt;% \n  mutate(\n    species = factor(\n      case_when(\n        id == \"1\" ~ \"ARCS\",\n        id == \"2\" ~ \"BDWF\",\n        id == \"3\" ~ \"LKWF\",\n        id == \"4\" ~ \"LSCS\",\n      ), level = c(\"ARCS\", \"BDWF\", \"LKWF\", \"LSCS\")\n    )\n  ) %&gt;% \n  as_tibble()\n\nWe will randomly sample 10 ellipses out of 1,000. You can change this but this seems pretty standard.\n\nellipse_df %&gt;% \n  group_by(species, rep) %&gt;% \n  nest() %&gt;%\n  group_by(species) %&gt;% \n  slice_sample(n = 10, replace = TRUE) %&gt;% \n  ungroup() %&gt;% \n  unnest(cols = c(data)) -&gt; random_ellipse \n\n\n\n0.9 Plot ellipses, densities of each istope, and isotope biplot\nWe will first plot the ellipse for each species\n\nellipse_plots &lt;- ggplot() + \n  geom_polygon(data = random_ellipse,\n               mapping = aes(x = d13c, y = d15n,\n                             group = interaction(rep, species),\n                             color = species),\n               fill = NA,\n               linewidth = 0.5) + \n  \n  scale_colour_viridis_d(begin = 0.25, end = 0.75, \n                         option = \"D\", name = \"species\",\n  ) + \n  scale_x_continuous(breaks = rev(seq(-20, -40, -2))) +\n  scale_y_continuous(breaks = seq(6, 16, 2)) +\n  theme_bw(base_size = 10) +\n  theme(axis.text = element_text(colour = \"black\"),\n        panel.grid = element_blank(), \n        legend.position = \"none\", \n        legend.title.align = 0.5,\n        legend.background = element_blank()) + \n  labs(x = expression(paste(delta ^ 13, \"C\")), \n       y = expression(paste(delta ^ 15, \"N\")))\n\nWarning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2\n3.5.0.\nℹ Please use theme(legend.title = element_text(hjust)) instead.\n\n\nWe need to turn df into long format to iterate over using imap() to easily create density plots. You will notice that I again use case_when() to make columns of element abbreviations and neutron numbers that will be used in plot labelling.\n\niso_long &lt;- df %&gt;%\n  pivot_longer(cols = -species,\n               names_to = \"isotope\", \n               values_to = \"value\") %&gt;% \n  mutate(\n    element = case_when(\n      isotope == \"d15n\" ~ \"N\",\n      isotope == \"d13c\" ~ \"C\",\n    ), \n    neutron = case_when(\n      isotope == \"d15n\" ~ 15,\n      isotope == \"d13c\" ~ 13,\n    )\n  )\n\nWe will then make density plots for each isotope using geom_density()\n\niso_density &lt;- iso_long %&gt;% \n  group_split(isotope) %&gt;% \n  imap(\n    ~ ggplot(data = .) + \n      geom_density(aes(x = value, \n                       fill = species), \n                   alpha = 0.35, \n                   linewidth = 0.8) +\n      scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                           option = \"D\", name = \"Species\") +\n      theme_bw(base_size = 10) +\n      theme(axis.text = element_text(colour = \"black\"),\n            panel.grid = element_blank(), \n            legend.position = c(0.15, 0.65), \n            legend.title.align = 0.5,\n            legend.background = element_blank(), \n            axis.title.x = element_markdown(family = \"sans\")) + \n      labs(x =  paste(\"\\U03B4\",\n                      \"&lt;sup&gt;\", unique(.$neutron), \"&lt;/sup&gt;\",unique(.$element), \n                      sep = \"\"), \n           y = \"Density\")\n  )\n\nd13c_density &lt;- iso_density[[1]] + \n  scale_x_continuous(breaks = rev(seq(-20, -34, -2)),\n                     limits = rev(c(-20, -34)))\n\nd15n_density &lt;- iso_density[[2]] +\n  scale_x_continuous(breaks = seq(5, 15, 2.5), \n                     limits = c(5, 15)) + \n  theme(\n    legend.position = \"none\"\n  )\n\nLastly we will use geom_point() to make isotopic biplot.\n\niso_biplot &lt;- ggplot() + \n  geom_point(data = df, aes(x = d13c, y = d15n,\n                            fill = species),\n             shape = 21, colour = \"black\", \n             stroke = 0.8,\n             size = 3, alpha = 0.70) +\n  scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                       option = \"D\", name = \"species\") +\n  scale_x_continuous(breaks = rev(seq(-20, -39, -1))) +\n  scale_y_continuous(breaks = seq(5, 17, 1)) +\n  theme_bw(base_size = 10) +\n  theme(axis.text = element_text(colour = \"black\"),\n        panel.grid = element_blank(), \n        legend.position = \"none\", \n        legend.title.align = 0.5,\n        legend.background = element_blank()) + \n  labs(x = expression(paste(delta ^ 13, \"C\")), \n       y = expression(paste(delta ^ 15, \"N\")))\n\n\n\n0.10 Use {patchwork} to make ellipse, density, and biplots into a paneled figure.\nWe can also use the function plot_annotation() to add lettering to the figure that can be used in the figure description. To maneuver where plot_annotation() places the lettering, we need to add plot.tag.position = c(x, y) to the theme() call in every plot.\n\nd13c_density + ellipse_plots + iso_biplot + d15n_density +\n  plot_annotation(tag_levels = \"a\", \n                  tag_suffix = \")\")\n\n\n\n\n\n\n\n\n\n\n0.11 Determine the 95% niche similarties for each species\nWe will use the overlap() function from {nicheROVER} to estimate the percentage of similarty among species. We will set overlap to assess based on 95% similarities.\n\nover_stat &lt;- overlap(fish_par, nreps = nsample, nprob = 1000, \n                     alpha = 0.95)\n\nWe then are going transform this output to a dataframe and make the dataframe long format for plotting so we can assess overall similarities among species.\n\nover_stat_df &lt;- over_stat %&gt;% \n  as_tibble(rownames = \"species_a\") %&gt;% \n  mutate(\n    id = 1:nrow(.), \n    species_a = factor(species_a, \n                       level = c(\"ARCS\", \"BDWF\", \"LKWF\", \"LSCS\"))\n  ) %&gt;% \n  pivot_longer(cols = -c(id, species_a), \n               names_to = \"species_b\", \n               values_to = \"mc_nr\")  %&gt;% \n  separate(species_b, into = c(\"species_c\", \"sample_number\"), \n           sep = \"\\\\.\") %&gt;% \n  select(-id) %&gt;% \n  rename(species_b = species_c) %&gt;% \n  mutate(\n    species_b =  factor(species_b, \n                        level = c(\"ARCS\", \"BDWF\", \"LKWF\", \"LSCS\")\n                        ), \n    mc_nr_perc = mc_nr * 100\n  )\n\nWe then are going to take our newly made data frame and extract out the mean percentage of similarities and the 2.5% and 97.5% quarantines. We plot these as lines and dotted lines on our percent similarity density figure.\n\nover_sum &lt;- over_stat_df %&gt;% \n  group_by(species_a, species_b) %&gt;% \n  summarise(\n    mean_mc_nr = round(mean(mc_nr_perc), digits = 2),\n    qual_2.5 = round(quantile(mc_nr_perc, probs = 0.025, na.rm = TRUE), digits = 2), \n    qual_97.5 = round(quantile(mc_nr_perc, probs = 0.975, na.rm = TRUE), digits = 2)\n  ) %&gt;% \n  ungroup() %&gt;% \n  pivot_longer(cols = -c(species_a, species_b, mean_mc_nr), \n               names_to = \"percentage\", \n               values_to = \"mc_nr_qual\") %&gt;% \n  mutate(\n    percentage = as.numeric(str_remove(percentage, \"qual_\"))\n  ) \n\nWe are now going to use ggplot(), geom_density(), and fact_grid2() from {ggh4x}.\n\nggplot(data = over_stat_df, aes(x = mc_nr_perc)) + \n  geom_density(aes(fill = species_a)) + \n  geom_vline(data = over_sum, aes(xintercept = mean_mc_nr), \n             colour = \"black\", linewidth = 1) +\n  geom_vline(data = over_sum, aes(xintercept = mc_nr_qual), \n             colour = \"black\", linewidth = 1, linetype = 6) +\n  scale_fill_viridis_d(begin = 0.25, end = 0.75,\n                       option = \"D\", name = \"Species\", \n                       alpha = 0.35) + \n  ggh4x::facet_grid2(species_a ~ species_b, \n                     independent = \"y\",\n                     scales = \"free_y\") + \n  theme_bw() + \n  theme(\n    panel.grid = element_blank(), \n    axis.text = element_text(colour = \"black\"), \n    legend.background = element_blank(),\n    strip.background = element_blank()\n  ) +\n  labs(x = paste(\"Overlap Probability (%)\", \"\\u2013\", \n                 \"Niche Region Size: 95%\"), \n       y = \"p(Percent Overlap | X)\")\n\n\n\n\n\n\n\n\n\n\n0.12 Estimate overall niche size\nWe are now going to estimate the overall size of the niche for each posterior sample by using the function niche.size().\n\nniche_size &lt;- sapply(fish_par, function(spec) {\n  apply(spec$Sigma, 3, niche.size)\n})\n\nWe then need to transform niche_size into a datafame for visualization and summary statistics.\n\nniche_size_df &lt;- niche_size %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    id = 1:nrow(.)\n  ) %&gt;% \n  pivot_longer(\n    cols = -id, \n    names_to = \"species\", \n    values_to = \"niche_size\"\n  ) %&gt;% \n  mutate(\n    id = 1:nrow(.), \n    species = factor(species,\n                   level = c(\"ARCS\", \"BDWF\", \n                             \"LKWF\", \"LSCS\"))\n  )\n\nWe can calculate the mean niche size, standard deviation, and standard error.\n\nniche_size_mean &lt;- niche_size_df %&gt;% \n  group_by(species) %&gt;% \n  summarise(\n    mean_niche = round(mean(niche_size), digits = 2), \n    sd_niche = round(sd(niche_size), digits = 2), \n    sem_niche = round(sd(niche_size) / sqrt(n()), digits = 2)\n  )\n\n\n\n0.13 Plot niche size\nWe will now use geom_violin(), geom_point(), and geom_errorbar() to plot the distribution for niche size for each species.\n\nggplot(data = niche_size_df) + \n  geom_violin(\n    aes(x = species, y = niche_size),\n    width = 0.2) + \n  geom_point(data = niche_size_mean, aes(x = species, y = mean_niche)) +\n  geom_errorbar(data = niche_size_mean, aes(x = species, \n                                            ymin = mean_niche  - sem_niche, \n                                            ymax = mean_niche  + sem_niche), \n                width = 0.05) +\n  theme_bw(base_size = 15) + \n  theme(panel.grid = element_blank(), \n        axis.text = element_text(colour = \"black\")) + \n  labs(x = \"Species\", \n       y = \"Niche Size\") \n\n\n\n\n\n\n\n\nNow that we have our niche sizes and similarities determined we can make inferences about the species, trophic similarities, and the ecosystem."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a fisheries scientist and programmer currently based at the Great Lakes Institute for Environmental Research working on fish spatial ecology and food web dynamics in Lake Ontario. This blog is an extension of my website and is where I host vignettes on a wide range of topics mostly focused on data applications for aquatic scientists.\nIf you’re looking for data applications related to human-dimensions, sports business, and sports management please check out Matthew Hlina’s website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "MLB - Market Ranking\n\n\n\n\n\n\nMLB\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use {nichetools} with SIBER\n\n\n\n\n\n\nStable Isotopes\n\n\nTrophic Dynamics\n\n\nFood Webs\n\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use {nichetools} with nicheROVER\n\n\n\n\n\n\nStable Isotopes\n\n\nTrophic Dynamics\n\n\nFood Webs\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate detection efficiency for acoustic telemetry receivers\n\n\n\n\n\n\nAcoustic Telemetry\n\n\nGLATOS\n\n\nGeneralized Linear Model\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal movement across a boundary\n\n\n\n\n\n\nSpatial Analysis\n\n\nAcoustic Telemetry\n\n\nMovement\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use {soapcheckr} to make soap-flim smoothers\n\n\n\n\n\n\nGeneralized Additive Model\n\n\nSpatial Analysis\n\n\nBathymetry\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Trophic Niches - {nicheROVER} and {ggplot2}\n\n\n\n\n\n\nStable Isotopes\n\n\nTrophic Dynamics\n\n\nFood Webs\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nShortest Paths Within a Boundary - {pathroutr}\n\n\n\n\n\n\nSpatial Analysis\n\n\nAcoustic Telemetry\n\n\nMovement\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\n\n\n\n\n\n\nShortest Paths Within a Boundary - {gdistance}\n\n\n\n\n\n\nSpatial Analysis\n\n\nAcoustic Telemetry\n\n\nMovement\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nBenjamin L. Hlina\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/shortest-path-pathroutr/shortest_path_example_pathroutr.html",
    "href": "posts/post-with-code/shortest-path-pathroutr/shortest_path_example_pathroutr.html",
    "title": "Shortest Paths Within a Boundary - {pathroutr}",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to create the shortest distance among acoustic telemetry receivers within a confined boundary such as a lake, river, delta, or oceanscape. This workflow can be adapted to find the distance between any two points within a confined boundary.\nUse this vignette with a bit of caution, as I found some inconsistency with this method when transferring it to other study systems besides this example study system.\nNote, this method differs from the {gdistance} method as we are going to create a network graph to move throughout our study system and determine the shortest path. This vignette will start off the same as the {gdistance} method but will differ when creating the shortest paths.\nYou can download and unzip this vignette using the following code:\n\ninstall.packages(\"usethis\")\nusethis::use_course(\"https://github.com/benjaminhlina/shortest_path_example/archive/refs/heads/master.zip\")\n\n\n\n0.2 Load shapefile and receiver locations\nWe will first load all the packages we need, we will use {pathroutr} to find the shortest paths using {sfnetworks} and {sf} to find the distances of those shortest paths.\n\n# ---- load packages ----\n{\n  library(dplyr)\n  library(ggplot2)\n  library(ggspatial)\n  library(here)\n  library(igraph)\n  library(lwgeom)\n  library(pathroutr)\n  library(purrr)\n  library(readr)\n  library(sf)\n  library(sfnetworks)\n  library(sp)\n  library(tibble)\n  library(tidyr)\n  make_line &lt;- function(lon, lat, llon, llat) {\n    st_linestring(matrix(c(lon, llon, lat, llat), 2, 2))\n  }\n}\n\nWe will bring bring in our shapefile. This vignette will use Big Sissabagama Lake as it is the lake I grew up fishing on in Wisconsin, USA. Please replace with the shapefile of your desired body of water.\n\nlake &lt;- st_read(dsn = here(\"Data\",\n                           \"shapefile\",\n                           \".\"),\n                layer = \"sissabagama_lake\")\n\nImportant that you convert to the correct UTM zone. For the vignette we are using UTM zone 15 N. Adjust your UTM zone accordingly.\n\nlake_utm &lt;- st_transform(lake, crs = 32615)\n\nWe will then bring in our receiver locations. Replace rl_sum_sf with your receiver locations as a RDS or csv file type or whatever you use to document receiver locations.\n\nrl_sum_sf &lt;- read_rds(here(\"Data\",\n                           \"receiver locations\",\n                           \"rl_sum_sf.rds\"))\n\nConvert to UTMs for plotting purposes and make sure you use the correct UTM zone.\n\nrl_sum_utm &lt;- st_transform(rl_sum_sf, crs = 32615)\n\n\n\n0.3 Invert shapefile for inland lakes and rivers\n{pathroutr} was built with the intent of working on oceanscapes where the shapefile is land. For inland bodies of water the shapefile is usually water, therefore to get {pathroutr} to function we need to invert our inland lake or river shapefile\nFirst we are going to get the extent of our shapefile which will be in UTMs\n\next &lt;- st_bbox(lake_utm, crs = st_crs(lake_utm)) %&gt;%\n  st_as_sfc() %&gt;%\n  st_sf()\n\nWe then will invert our shapefile by using st_difference() from {sf}.\n\ninverse &lt;- st_difference(ext, lake_utm)\n\nWe will check if we have correctly taken the inverse of our lake.\n\nggplot() +\n  geom_sf(data = inverse) + \n  theme_void()\n\n\n\n\n0.4 Create land region to build our network\nWe need to create a buffered land region to use as a barrier. Within st_buffer() we will need to adjust dist argument to change the buffer distance to be adequate for the study system. For this example we will use 650 m.\n\nland_region &lt;- rl_sum_utm %&gt;% \n  st_buffer(dist = 650) %&gt;%\n  st_union() %&gt;%\n  st_convex_hull() %&gt;% \n  st_intersection(inverse) %&gt;% \n  st_sf()\n\nWe will check if we have correctly buffered our land region\n\nggplot() +\n  geom_sf(data = land_region) + \n  theme_void()\n\n\n\n\n0.5 Create every combination of paths for every receiver\nFirst we will convert receiver location which is a sf object to a tibble with each location combination.\n\nprep_path &lt;- rl_sum_sf %&gt;%\n  mutate(\n    lon = st_coordinates(.)[,\"X\"],# grab lon\n    lat = st_coordinates(.)[,\"Y\"],# grab lat\n  ) %&gt;%  \n  st_drop_geometry() %&gt;% # drop sf \n  # once geometry removed create to and from lat longs \n  mutate(llon = lon,\n         llat = lat,\n         lonlat = paste0(lon, \",\", lat),\n         llonllat = paste0(llon, \",\", llat)) %&gt;%\n  dplyr::select(-lon, -lat, -llon, -llat) %&gt;%\n  expand(lonlat, llonllat) %&gt;% # expand for each to and from combo \n  separate(lonlat, c(\"lon\", \"lat\"), \",\") %&gt;%\n  separate(llonllat, c(\"llon\", \"llat\"), \",\")\n\nprep_path has all of the path combinations but we lose the names of the receivers and which paths go from one receiver to another. We are going to add that information back in by creating an object called rec_order\n\nrec_order &lt;- prep_path %&gt;%\n  left_join(\n    rl_sum_sf %&gt;% \n      mutate(\n        lon = st_coordinates(.)[,\"X\"], \n        lat = st_coordinates(.)[,\"Y\"]\n      ) %&gt;% \n      st_drop_geometry() %&gt;% \n      rename(from = rec_name) %&gt;% \n      dplyr::select(from, lon, lat) %&gt;% \n      mutate(across(.cols = c(lon, lat), as.character)), by = c(\"lon\", \"lat\"), \n    multiple = \"all\"\n  ) %&gt;%  \n  left_join(\n    rl_sum_sf %&gt;% \n      mutate(\n        lon = st_coordinates(.)[,\"X\"]\n      ) %&gt;% \n      st_drop_geometry() %&gt;% \n      rename(to = rec_name,\n             llon = lon) %&gt;% \n      dplyr::select(to, llon) %&gt;% \n      mutate(llon = as.character(llon)), by = c(\"llon\"), \n    multiple = \"all\"\n  ) %&gt;% \n  mutate(\n    from_to = paste0(from, \"-\", to)\n  ) %&gt;% \n  dplyr::select(from, to, from_to, lon, lat, llon, llat)\n\nAwesome! We have all of our combinations with their names and we now know which paths go from one receiver to another. Now we need to make each combination a linestring that we will sample points from to reroute.\nBe sure to choose the correct UTM zone here. This vignette uses UTM zone 15 north but for other uses you will have to change the UTM zone.\n\npath &lt;- prep_path %&gt;%\n  mutate(across(lon:llat, as.numeric)) %&gt;% \n  pmap(make_line) %&gt;%\n  st_as_sfc(crs = 4326) %&gt;%\n  st_sf() %&gt;%  \n  mutate(\n    lon = st_startpoint(.) %&gt;%\n      st_coordinates(.) %&gt;%\n      as_tibble() %&gt;%\n      .$X %&gt;% \n      as.character(),\n    llon = st_endpoint(.) %&gt;%\n      st_coordinates(.) %&gt;%\n      as_tibble() %&gt;%\n      .$X %&gt;% \n      as.character()\n  ) %&gt;% \n  left_join(rec_order %&gt;%\n               dplyr::select(from:lon, llon),\n             by = c(\"lon\", \"llon\")\n  ) %&gt;%\n  dplyr::select(from:from_to) %&gt;% \n  st_transform(crs = 32615) %&gt;% \n  arrange(from, to)\n\n\n\n0.6 Sample points along path for {pathroutr} to reroute\nNow that we have our paths we need to sample along the LINESTRING to get paths to reroute. I choose to have a sample distance of 5 m but you can change this depending on the study site.\nWe need to cast our sampled points as MULTIPOINT object for {pathroutr} to sample\n\npath_pts &lt;- path %&gt;% \n  st_segmentize(dfMaxLength = units::set_units(5, m)) %&gt;% \n  st_cast(\"MULTIPOINT\")\n\nImportant note, sometimes this sampling step results in points that intersect with the boundary of your land region which may cause issues with {pathroutr}. To fix this you will need to use the following code:\n\npath_pts_fix &lt;- prt_trim(trkpts = path_pts, barrier = land_region)\n\nThough for this vignette we will not use path_pts_fix.\nWe are going to visualize our paths, we could visualize the path_pts but this often takes awhile to process as the sample interval is 5 m and path_pts will be taken from our paths\n\nggplot() + \n  geom_sf(data = land_region)  +\n  geom_sf_label(data = rl_sum_sf, aes(label = rec_name), \n                size = 4) +\n  geom_sf(data = path, linewidth = 1) + \n  theme_void()\n\n\n\n\n0.7 Use get_barrier_segments from {pathroutr} to id path points that travel across landmasses in our lake.\n\nlake_bar_seg &lt;- get_barrier_segments(trkpts = path_pts,\n                                     barrier =  land_region)\n\n\n\n0.8 Create network graph in our study system\nCreate a network graph that is bound by land_region and made up Delaunay triangles which are created in theory by looking from one point on one shore to directly across the study system until you hit a land object.\n\nvis &lt;- prt_visgraph(barrier = land_region)\n\nWe can manipulate our network graph in several ways, the first is supplying the argument aug_points with the receiver location sf object. By doing so we add in triangles that will directly go to our receiver locations. Secondly, we can use the argument centroids to add additional points within our network.\nLastly, we can use the argument buffer to have paths be buffered from our land masses by a given distance in metres. We can visualize our network by first using the function activate from {sfnetworks}.\n\nvis_graph_sf &lt;- activate(vis, \"edges\") %&gt;% \n  st_as_sf()\n\nggplot() + \n  geom_sf(data = vis_graph_sf) + \n  theme_void()\n\n\n\n\n0.9 Create rerouted sections\nUsing prt_shortestpath we will reroute sections that go across land using our created network.\n\nsegs_tbl &lt;- prt_shortpath(lake_bar_seg, vis, blend = TRUE)\n\nWe can visualize these rerouted paths\n\nggplot() + \n  geom_sf(data = land_region, size = 0) +\n  layer_spatial(data = segs_tbl$geometry, \n                color = \"deepskyblue3\",\n                linewidth = 1) +\n  theme_void()\n\n\n\n\n0.10 Update our paths to not travel across land\n\ntrack_pts_fix &lt;- prt_reroute(path_pts, land_region, vis)\n\ntrack_pts_fix &lt;- prt_update_points(track_pts_fix, path_pts)\n\n\n\n0.11 Convert points to linestrings to determine distance\n\ntrack_pts_fixed &lt;- track_pts_fix %&gt;% \n  group_by(from_to) %&gt;% \n  summarise(do_union = FALSE) %&gt;% \n  st_cast('LINESTRING') %&gt;%  \n  ungroup() %&gt;% \n  separate(from_to, into = c(\"from\", \"to\"), sep = \"-\", \n           remove = FALSE) %&gt;% \n  mutate(\n    cost_dist_m = as.numeric(st_length(.))\n  ) %&gt;% \n  filter(from != to) %&gt;% \n  dplyr::select(from, to, from_to, cost_dist_m, geometry)\n\n\n\n0.12 Visualize our paths\nFirst we will check if one route rerouted properly\n\n# view one reroute to confirm pathroutr is rerouting \ntrack_pts_fixed %&gt;% \n  filter(from_to == \"15-11\") %&gt;% \n  ggplot() + \n  geom_sf(data = land_region, size = 0) +\n  geom_sf(color = \"deepskyblue3\", \n          linewidth = 1) +\n  theme_void()\n\n\nNext we will visualize the whole network and have colour be our distances\n\nggplot() + \n  geom_sf(data = lake_utm, colour = \"black\",\n          size = 1) +\n  geom_sf(data = rl_sum_utm, \n          size = 4, colour = \"black\") + \n  geom_sf(data = track_pts_fixed, \n          aes(color = cost_dist_m), \n          linewidth = 1) +\n  scale_colour_viridis_c(option = \"D\", \n                         name = \"Cost Distance (m)\") + \n  theme_void()\n\n From here the sf object can be kept together or ripped apart to determine the distance or path a fish could swim within the system along with a whole host of other potential implications (e.g. interpolated paths)."
  },
  {
    "objectID": "posts/post-with-code/find-ts-bnd/index.html",
    "href": "posts/post-with-code/find-ts-bnd/index.html",
    "title": "Animal movement across a boundary",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to determine when (i.e., time stamps), where (i.e., nodes) and how long, an animal takes to cross a boundary (i.e., line or polygon). The rational for wanting to determine time spent crossing a boundary could be multiple situations. For example, wanting to estimate the amount of time it took for a fish to migrate into or out of a marine protected area (MPA) or a commercial fishing zone in Lake Michigan using satellite or acoustic telemetry data. This vignette was inspired the following twitter post with the initial ideas of a solution inspired by the following blog post by Jesse Sadler focused on network analysis. Additional chunks of code came from Shortest Paths Within a Boundary - {pathroutr} and within functions in {soapcheckr}. Another interesting note is that this problem might be solved with functions from {lwgeom}, {sftime}, {move}, {crawl}, {aniMotum, {trackeR}, and {sftracks}. I don’t have as much experience using serveral of these packages but they might provide an easier workflow to answer this problem.\nYou can download and unzip this vignette using the following code:\n\ninstall.packages(\"usethis\")\nusethis::use_course(\"https://github.com/benjaminhlina/find-ts-bnd/archive/refs/heads/master.zip\")\n\n\n\n0.2 load packages, data, and boundary\nWe will first load all the packages we need, we will use {lwgeom} to extract lat and lon of each change in direction using st_startpoint() and st_endpoint() and {sf} to find tracks that cross our boundary.\n\n{\n  library(dplyr)\n  library(ggplot2)\n  library(here)\n  library(lwgeom)\n  library(purrr)\n  library(readr)\n  library(sf)\n  make_line &lt;- function(lon, lat, llon, llat) {\n    st_linestring(matrix(c(lon, llon, lat, llat), 2, 2,))\n  }\n}\n\nNext we will bring in our example detection data for a single fish that was tagged with a satellite or acoustic telemetry transmitter.\n\ndat &lt;- read_csv(here(\"data\", \n                     \"example_movement_data.csv\"))\nglimpse(dat)\n\nFor initial plotting purposes we will transform our detection data into a sf object that is a compilation of all the movements into a LINESTRING. We will only use dat_sf for visualization and not data manipulation to answer this problem.\n\ndat_sf &lt;- dat %&gt;% \n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %&gt;% \n  group_by(id) %&gt;% \n  summarise(do_union = FALSE) %&gt;% \n  st_cast(\"LINESTRING\")\n\nNext we will bring in our example MPA shapefile as a sf object. For this exercise the boundary needs to be either a LINESTRING or MULTILINESTRING as POLYGON and MULTIPOLYGON will not work because the area inside the polygon is considered filled.\n\nbnd &lt;- st_read(dsn = here(\"data\", \n                          \"shapefile\", \n                          \".\"), \n               layer = \"example_mpa\") %&gt;% \n  st_cast(\"MULTILINESTRING\")\n\n\n\n0.3 Intial plot of movement\nWe will use {ggplot2} and geom_sf() to initially visualize our tracks.\n\nggplot() + \n  geom_sf(data = bnd, fill = NA, colour = \"blue\", linewidth = 1) + \n  geom_sf(data = dat_sf) + \n  theme_void()\n\n\n\n\n\n\n\n\nWe can notice that there are few times the fish moves from outside to inside of the MPA or vis versa. We want to know the location of those two nodes and the time between the detections to know how long it took the fish to move from outside to inside and vis versa.\n\n\n0.4 Create our from and to data frame\nWe need to take the detection data and split it up so that each subsequent detection time stamp, latitude, and longitude are in a separate column. We can do this by using the lag() function from either {dplyr} or {data.table}. We will also set the argument default for dplyr::lag() to equal first(YOUR_VARIABLE_NAME) so when lagging back to the previous value for the first time, lag() does not create a NA value.\n\nto_from &lt;- dat %&gt;% \n  mutate(\n    from_ts = dplyr::lag(time_stamp, default = first(time_stamp)),\n    to_ts = time_stamp,\n    llat = dplyr::lag(lat, default = first(lat)),\n    llon = dplyr::lag(lon, default = first(lon)),\n  ) %&gt;% \n  dplyr::select(-time_stamp)\n\n\n\n0.5 Create our tracks for each change in direction\nNow that we have our to and from dataframe we are going to select the from and to latitudes and longitudes and make each movement a LINESTRING using the supplied function above make_line(). We will then convert that into a complied sf object and extract the longitudes from each movement LINESTRING using st_startpoint() and st_endpoint() from {lwgeom}. We will add this as a column to our sf object and transform it to a character because the next step involves left_join() from {dplyr} which cannot join based on a double precision numeric.\n\ntracks_sf &lt;- to_from %&gt;%\n  dplyr::select(lon, lat, llon,llat) %&gt;% \n  pmap(make_line) %&gt;%\n  st_as_sfc(crs = 4326) %&gt;%\n  st_sf() %&gt;% \n  mutate(\n    lon = st_startpoint(.) %&gt;%\n      st_coordinates(.) %&gt;%\n      as_tibble() %&gt;%\n      .$X %&gt;% \n      as.character(),\n    llon = st_endpoint(.) %&gt;%\n      st_coordinates(.) %&gt;%\n      as_tibble() %&gt;%\n      .$X %&gt;% \n      as.character()\n  )\n\n\n\n0.6 Join our tracks to our from and to timestamps\nFirst we need to make sure our from and to latitudes and longitudes are characters otherwise left_join() will not work. We will also select the columns we only need to join. If you want to know the location of each node you can select all from and to latitudes and longitudes.\n\nto_from_select &lt;- to_from %&gt;% \n  mutate(across(.cols = c(lon, lat, llon, llat), as.character)) %&gt;% \n  dplyr::select(id, lon, llon, from_ts, to_ts)\n\nNext we will left_join() our tracks sf object to our timestamps and fish id dataframe lastly we will drop the lon and llon columns as we no longer need them in our sf object unless you want the location of each from and to node.\n\ntracks_sf &lt;- tracks_sf %&gt;% \n  left_join(to_from_select,\n            by = c(\"lon\", \"llon\")) %&gt;% \n  dplyr::select(-c(\"lon\", \"llon\"))\n\n\n\n0.7 Determine the movements that fall outside the boundary\nWe can use st_intersects() with the sparse argument set to FALSE. This will produce a matrix of TRUE or FALSE depending on whether each LINESTRING intersects the boundary. To return a vector of TRUE/FALSE instead of a matrix we add the [TRUE] at the end of the call of st_intersects().\n\ntracks_sf &lt;- tracks_sf %&gt;% \n  mutate(\n    x_bnd = st_intersects(bnd, tracks_sf, sparse = FALSE)[TRUE]\n  )\n\nCongratulations! We now have a column of TRUE/FALSE that lets us know whether or not the fish crossed the boundary. We can then filter out any movement that didn’t cross the boundary.\n\ntracks_sf_x &lt;- tracks_sf %&gt;% \n  filter(x_bnd %in% TRUE)\n\n\n\n0.8 Plotting tracks that cross the boundary\nWe can use {ggplot} or {mapview} to view when the fish moved across the boundary. {mapview} is nice to create an interactive plot for exploration but for any type of publication or report you’ll more than likely need a static figure and can produce it using {ggplot}.\n\nggplot() + \n  geom_sf(data = bnd, colour = \"blue\", size = 1, fill = NA) + \n  geom_sf(data = tracks_sf_x) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n0.9 Calculate time it took to cross the boundary\nWe can use the base function difftime() to get the time difference between our from and to detections. By calculating the time difference we now know the duration it took the fish to migrate across the boundary.\n\ntracks_sf_x &lt;- tracks_sf %&gt;% \n  mutate(\n    time_diff = difftime(to_ts, from_ts)\n  )\n\nYou will notice that the difference in time in this example is consistent because the example dataset uses a manufactured sequence that is equally spaced. In your dataset this column should vary depending on how the fish moved.\nCongratulations! You have now determined when, where, and how long it took for a fish to migrate across a boundary!"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html",
    "href": "posts/post-with-code/detection-efficiency/index.html",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "",
    "text": "This vignette describes methods to quickly and easily calculate the distance from a receiver that a given percentage (e.g., 50%) of detections are heard. The preliminary analysis will allow for range transmitters to be deployed for longer duration to estimate detection efficiency over a study period.\nThe preliminary study design/protocol is the following:\n\nDeploy range transmitter at set distances (e.g., 100, 250, 500, and 750 m) from the receiver you are wanting to range test for 24 hr. The range transmitters used in this vignette had a min delay of 840 s and max delay of 960 s but you could use continuous transmitters or transmitter with different delays.\nAfter 24 hr retrieve transmitters and receivers and download vrl files.\nImport vrl files and receiver and transmitter location data into Fathom Central. Unfortunately you will need internet to do this. If you are unable to use the internet (e.g., in the field), download an older version of the detection efficiency software produced by Innovasea and import the vrl and receiver and transmitter location data. The manual for this software is quite good and will walk you through how to get your data into the detection efficiency tool. You can download the older version at the following Innovasea website. We can also determine detection efficiency in R as well. All we need is the detection csv and receiver and transmitter location data. You can also calculate initial detection efficiency using R, there is a section below that will describe how to do this.\nHighly suggest doing multiple 24 hr deployments at the same distances and/or drifts. If you have multiple sets of detection efficiency you will be able to create a model that fits the data better, thus ultimately improving the redeployment locations of range transmitters for longer duration (e.g., length of study).\nCalculate the detection efficiency for each distance over the 24 hr using detection range calculator linked above or in fathom central and export the csv.\nUse the exported csv to estimate the detection efficiency at a given distance using detection_range_model() from {glatos}. The function, detection_range_model(), will return an estimated distance away from the receiver for a given detection efficiency (e.g., 50%).\nWe can use R, Python, or GIS to create redeployment locations (i.e., latitude and longitude) at the distances the model estimates for the desired detection efficiency (e.g., 50%).\nWe can redeploy our range transmitters for a given duration (e.g., 6 month, 1 year), to determine changes in detection efficiency over the study period.\n\nThe code below will walk through how to use the detection efficiency data produced by Innovasea software to estimate the redeployment distance away from the receiver and create the redeployment location."
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#load-packages-and-data",
    "href": "posts/post-with-code/detection-efficiency/index.html#load-packages-and-data",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "3.1 Load packages and data",
    "text": "3.1 Load packages and data\nWe will first load the desired packages in R. We will load {dplyr} for data manipulation processes, {ggplot2} to plot our models, {glatos} to use multiple functions associated with acoustic telemetry including modelling detection range, {mapview} to visualize the deployment and redeployment data, {purrr} to iterate over processes, and {sf} to work with spatial data.\n\n# ---- Bring in R packages ----\n{\n  library(dplyr)\n  library(ggplot2)\n  library(glatos)\n  library(mapview)\n  library(purrr)\n  library(sf)\n}\n\nWarning: package 'glatos' was built under R version 4.4.1\n\n\nWe will work with the example data set in {glatos} but the receiver deployment data either in table format or as sf object and a shapefile of the body of water you’re working on.\n\n# get path to example receiver_locations file\nrec_file &lt;- system.file(\"extdata\",\n  \"sample_receivers.csv\",\n  package = \"glatos\"\n)\n\n# note that code above is needed to find the example file\n# for real glatos data, use something like below\n# rec_file &lt;- \"c:/path_to_file/GLATOS_receiverLocations_20150321_132242.csv\"\n\nrcv &lt;- read_glatos_receivers(rec_file)\n\nglimpse(rcv)"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#convert-to-sf-object",
    "href": "posts/post-with-code/detection-efficiency/index.html#convert-to-sf-object",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "3.2 Convert to sf object",
    "text": "3.2 Convert to sf object\nWe will convert deployment data to a sf object and filter the data to only use the glatos array \"OSC\" for the example. You will not have to do the filtering when using your data but you will want to convert your receiver locations to a sf object and display using mapview().\n\nrcv_osc_sf &lt;- rcv %&gt;%\n  st_as_sf(\n    coords = c(\"deploy_long\", \"deploy_lat\"),\n    crs = 4326\n  ) %&gt;%\n  filter(glatos_array %in% \"OSC\")\n\n# view in mapview\nmapview(rcv_osc_sf)\n\n\n\n\n\nWe will filter out station number 12 as example to create our intial deployment rings. We will also convert into UTMs so we can estimate distances away from the receiver in meters.\n\nrcv_osc_sf_12 &lt;- rcv_osc_sf %&gt;%\n  filter(station_no %in% 12) %&gt;%\n  st_transform(crs = 32617)"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#create-buffer-rings-at-set-distances",
    "href": "posts/post-with-code/detection-efficiency/index.html#create-buffer-rings-at-set-distances",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "3.3 Create buffer rings at set distances",
    "text": "3.3 Create buffer rings at set distances\nNext, we will create buffer rings at our set distances away from the receiver. These distances are 100, 250, 500, and 750 but feel free to add more distances. If you add more distances, the model predictions will likely be more accurate.\n\n# first create a data frame of distances to iterate over\n\ndists &lt;- data.frame(\n  distance = c(100, 250, 500, 750)\n)\n# next we will split the data frame by distance and iterate over it using map\nbuffer_rings &lt;- dists %&gt;%\n  split(.$distance) %&gt;%\n  map(~ st_buffer(dist = .x$distance, rcv_osc_sf_12)) %&gt;%\n  bind_rows(.id = \"distance\") %&gt;%\n  st_cast(\"LINESTRING\", warn = FALSE) %&gt;%\n  dplyr::select(distance, glatos_array, station_no, ins_serial_no, geometry)\n# now view buffer rings\nmapview(rcv_osc_sf) +\n  mapview(buffer_rings)\n\n\n\n\n\nNow that we have the buffer rings, we are going to pick 3 locations for each distance to potentially deploy range transmitters for 24 hr. We will then create an excel and gpx file of all the locations.\nFirst we transform our buffer rings into spatial points, view them using mapview() and filter out the points we want. Then save as an excel and/or gpx file.\n\nbuffer_rings_pts &lt;- buffer_rings %&gt;%\n  st_cast(\"POINT\", warn = FALSE) %&gt;%\n  mutate(\n    id = 1:nrow(.)\n  ) %&gt;%\n  dplyr::select(id, distance:geometry)\n\nView them in mapview, this part will might take a little bit to select each point/figure out which ones you want.\n\nmapview(rcv_osc_sf) +\n  mapview(buffer_rings_pts)"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#select-locations-for-deployment",
    "href": "posts/post-with-code/detection-efficiency/index.html#select-locations-for-deployment",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "3.4 Select locations for deployment",
    "text": "3.4 Select locations for deployment\nThen filter out the points we want, this will change depending on your study site and what locations you want. We will transform the projection back to WGS 84 as this is likely what your gps or sonar will want. We will also add in a few columns to conform to GLATOS and OTN data standards and rearrange the column order.\n\ndeploy_sites &lt;- buffer_rings_pts %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  filter(id %in% c(\n    4, 49, 155,\n    407, 456, 472,\n    671, 696, 720,\n    835, 876, 920\n  )) %&gt;%\n  rename(\n    receiver_serial_no = ins_serial_no\n  ) %&gt;%\n  mutate(\n    deploy_date_time = NA,\n    deploy_lat = NA,\n    deploy_long = NA,\n    bottom_depth = NA,\n    riser_length = NA,\n    instrument_depth = NA,\n    ins_model_number = NA,\n    ins_serial_no = NA,\n    transmitter = NA,\n    transmitter_model = NA,\n    deployed_by = NA,\n    recovered = NA,\n    recover_date_time = NA,\n    recover_lat = NA,\n    recover_long = NA,\n    data_downloaded = NA,\n    download_date_time = NA,\n    comments = NA,\n    expect_deploy_lat = st_coordinates(.)[, \"Y\"],\n    expect_deploy_long = st_coordinates(.)[, \"X\"],\n    comments = NA\n  ) %&gt;%\n  dplyr::select(\n    id:receiver_serial_no,\n    deploy_date_time:expect_deploy_long, geometry\n  )"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#export-to-excel-and-gpx-formats",
    "href": "posts/post-with-code/detection-efficiency/index.html#export-to-excel-and-gpx-formats",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "3.5 Export to excel and gpx formats",
    "text": "3.5 Export to excel and gpx formats\nThen we will save as an excel and gpx file. To save as excel we will use {openxlsx}. You will notice that I don’t have it in the load packages area of this vignette. I have not placed it there for the purpose of how R builds vignettes but please add it to your load packages call. You will also need to replace \"YOUR_FILE_PATH\" with your file path for both saving as an excel and/or gpx file.\n\n# drop geometry (coerce to data.frame)\ndeploy_sites_df &lt;- st_drop_geometry(deploy_sites)\n\n# write xlsx file\nopenxlsx::write.xlsx(deploy_sites_df, \"YOUR_FILE_PATH.xlsx\")\n\n# save as gpx\nst_write(deploy_sites, \"YOUR_FILE_PATH.gpx\",\n  driver = \"GPX\",\n  dataset_options = \"GPX_USE_EXTENSIONS=YES\"\n)\n\nNow that you’ve created the excel and gpx file you can head out in the field to deploy your ranges transmitters for a given amount of time (e.g., 24 hr). After the time period you will retriever your receivers, download the vrl files and bring them into fathom central or the older detection range software. Once you export the csv you can move on to the next part of this vignette which is the analysis side."
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#preliminary-data-download",
    "href": "posts/post-with-code/detection-efficiency/index.html#preliminary-data-download",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "3.6 Preliminary data download",
    "text": "3.6 Preliminary data download\nImport vrl files and receiver and transmitter location data into Fathom Central. Unfortunately you will need internet to do this. If you are unable to use the internet (e.g., in the field), download an older version of the detection efficiency software produced by Innovasea and import the vrl and receiver and transmitter location data. The manual for this software is quite good and will walk you through how to get your data into the detection efficiency tool. You can download the older version at the following Innovasea website. Alternatively, you can calculate preliminary detection efficiency in R.\nTo do this in R can either read in each vrl file into R using {glatos} or {rvdat} or create a vue database, bring in all vrl files, and export the detection file as a csv.\nNext, add in a column that is the number of detections expected to be heard in 24 hr for the given delays. To do this we can take the number of seconds in a day, 86400 s, divided by average delay which in this case is 900 s. This value, 96, means that we should hear 96 detections in a day if a receiver heard all detections.\nWe can use the following example code to determine the preliminary percentage of detections that were heard at the set distances.\n\ndet_summary &lt;- dets %&gt;%\n  group_by(station, dets_expected, tag_serial_name) %&gt;%\n  summarise(\n    dets_heard = n()\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    dets_eff = dets_heard / dets_expected,\n    dets_eff_perc = (dets_heard / dets_expected) * 100\n  )"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#load-packages-and-data-1",
    "href": "posts/post-with-code/detection-efficiency/index.html#load-packages-and-data-1",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "4.1 Load packages and data",
    "text": "4.1 Load packages and data\n\n# ---- Bring in R packages ----\n{\n  library(dplyr)\n  library(ggplot2)\n  library(glatos)\n  library(mapview)\n  library(purrr)\n  library(sf)\n}\n\nConsidering we will be starting a new script and you will need the location of the receiver we will bring this information back in. For this example I will bring in the receiver location data that we used in the first half of the vignette\n\n# get path to example receiver_locations file\nrec_file &lt;- system.file(\"extdata\",\n  \"sample_receivers.csv\",\n  package = \"glatos\"\n)\n\n# note that code above is needed to find the example file\n# for real glatos data, use something like below\n# rec_file &lt;- \"c:/path_to_file/GLATOS_receiverLocations_20150321_132242.csv\"\n\nrcv &lt;- read_glatos_receivers(rec_file)\n\nglimpse(rcv)\n\nRows: 898\nColumns: 23\n$ station               &lt;chr&gt; \"WHT-009\", \"FDT-001\", \"FDT-004\", \"FDT-003\", \"FDT…\n$ glatos_array          &lt;chr&gt; \"WHT\", \"FDT\", \"FDT\", \"FDT\", \"FDT\", \"DTR\", \"DTR\",…\n$ station_no            &lt;chr&gt; \"9\", \"1\", \"4\", \"3\", \"2\", \"1\", \"2\", \"3\", \"4\", \"1\"…\n$ consecutive_deploy_no &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5, 2, 1, 2, 2, 1, …\n$ intend_lat            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ intend_long           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ deploy_lat            &lt;dbl&gt; 43.74216, 45.93014, 45.94764, 45.93794, 45.92377…\n$ deploy_long           &lt;dbl&gt; -82.50791, -83.50204, -83.48847, -83.46884, -83.…\n$ recover_lat           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ recover_long          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ deploy_date_time      &lt;dttm&gt; 2010-09-22 18:05:00, 2010-11-12 15:07:00, 2010-…\n$ recover_date_time     &lt;dttm&gt; 2012-08-15 16:52:00, 2012-05-15 13:25:00, 2012-…\n$ bottom_depth          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.5,…\n$ riser_length          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.5,…\n$ instrument_depth      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, N…\n$ ins_model_no          &lt;chr&gt; \"VR2W\", \"VR3\", \"VR3\", \"VR3\", \"VR3\", \"VR3\", \"VR3\"…\n$ glatos_ins_frequency  &lt;int&gt; 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, …\n$ ins_serial_no         &lt;chr&gt; \"109450\", \"442\", \"441\", \"444\", \"447\", \"439\", \"44…\n$ deployed_by           &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", …\n$ comments              &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", …\n$ glatos_seasonal       &lt;chr&gt; \"NO\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", …\n$ glatos_project        &lt;chr&gt; \"HECWL\", \"DRMLT\", \"DRMLT\", \"DRMLT\", \"DRMLT\", \"DR…\n$ glatos_vps            &lt;chr&gt; \"NO\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", …"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#convert-to-sf-object-1",
    "href": "posts/post-with-code/detection-efficiency/index.html#convert-to-sf-object-1",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "4.2 Convert to sf object",
    "text": "4.2 Convert to sf object\nWe will convert deployment data to a sf object and filter the data to only use the glatos array \"OSC\" for the example. You will not have to do the filtering when using your data but you will want to convert your receiver locations to a sf object and display using mapview().\n\nrcv_osc_sf &lt;- rcv %&gt;%\n  st_as_sf(\n    coords = c(\"deploy_long\", \"deploy_lat\"),\n    crs = 4326\n  ) %&gt;%\n  filter(glatos_array %in% \"OSC\")\n\n# view in mapview\nmapview(rcv_osc_sf)\n\n\n\n\n\nWe will filter out station number 12 as example to create our intial deployment rings. We will also convert into UTMs so we can estimate distances away from the receiver in meters.\n\nrcv_osc_sf_12 &lt;- rcv_osc_sf %&gt;%\n  filter(station_no %in% 12) %&gt;%\n  st_transform(crs = 32617)\n\nNext, we will bring in our example data which is loaded with {glatos} but you will need to replace sample_detection_efficiency with your data frame either by loading the csv produced by Innovasea software. You can do this multiple ways, I prefer using readr::read_csv() but base R works perfectly fine.\n\n# ----- uncomment the lines below to bring in your data ----\n#\n# and replace with the file path and name of detection efficiency\n# file (replace \"YOUR_DET_EFF.csv\")\n#\n# det_eff &lt;- readr::read_csv(\"YOUR_DET_EFF.csv\")\n#\n# glimpse(det_eff)\n\n# view sample detection efficiency data\n\nsample_detection_efficiency\n\nglimpse(sample_detection_efficiency)"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#calculate-distances",
    "href": "posts/post-with-code/detection-efficiency/index.html#calculate-distances",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "4.3 Calculate distances",
    "text": "4.3 Calculate distances\nNext we will use detection_range_model() to produce estimated distances for particular percentage (e.g., 50%). You will want to look through the help page for the function to make sure you’re setting up the model correctly.\nFew additional tips:\n\nWith fewer data points a third order polynomial often fits the data better, however, this does not mean that either a logit or probit model should not be assessed.\nIf a third order polynomial model is selected, the formula call can be in two different formats. The preferred and default format is y ~ -1 + x + I(x ^ 2) + I(x ^ 3) + offset(y-intercept), therefore, model_frame argument needs to be set \"data_frame\", which is the default, to properly extract parameters and determine distances from a receiver for the percentage of interest. If using the base::poly() in the formula such as, y ~ -1 + poly(x, 3, raw = TRUE) + offset(y-intercept), then, model_frame argument needs to be set to \"matrix\". Both formula formats have offset() which sets the y-intercept. The y-intercept needs to be set to 100, as x needs to equal 0 m from a receiver because you expect to hear a tag 100% of the time.\nA third order polynomial will handle preliminary detection efficiency percentages (y variable) as whole numbers as the model is not bound by 0 and 1. While both logit and probit models use percentages as decimals as the models are bound by 0 and 1.\n\nFirst, we will use a third order polynomial.\n\n# third order polynomial: ave_percent is a whole number\nm &lt;- detection_range_model(\n  avg_percent ~ -1 + distance_m + I(distance_m^2) +\n    I(distance_m^3) + offset(intercept),\n  data = sample_detection_efficiency,\n  percentage = c(10, 50, 90),\n  link = \"polynomial\",\n  model_frame = \"data_frame\"\n)\n\nWarning: Check if your formula is correct for the model_frame argument\n\n\nSecond, we will model the same data using a logit and probit model.\n\n# logit model: aver percent is in decimal form\n\nm1 &lt;- detection_range_model(avg_percent_d ~ distance_m,\n  data = sample_detection_efficiency,\n  percentage = c(10, 50, 90),\n  link = \"logit\",\n  summary_stats = TRUE\n)\n\n# probit model: aver percent is in decimal form\n\nm2 &lt;- detection_range_model(avg_percent_d ~ distance_m,\n  data = sample_detection_efficiency,\n  percentage = c(10, 50, 90),\n  link = \"probit\",\n  summary_stats = TRUE\n)\n\nWe will then view each of the results with the first being the third order polynomial.\n\nm\n\n# A tibble: 3 × 19\n      p distance    df chi_square      pgof     a  a_se a_sig        b     b_se\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    10      634     4      1304. 4.20e-281 0.202 0.183 0.332 -0.00136 0.000753\n2    50      371     4      1304. 4.20e-281 0.202 0.183 0.332 -0.00136 0.000753\n3    90      217     4      1304. 4.20e-281 0.202 0.183 0.332 -0.00136 0.000753\n# ℹ 9 more variables: b_sig &lt;dbl&gt;, d &lt;dbl&gt;, d_se &lt;dbl&gt;, d_sig &lt;dbl&gt;,\n#   offset &lt;dbl&gt;, resid_se &lt;dbl&gt;, r2 &lt;dbl&gt;, adj_r2 &lt;dbl&gt;, aic &lt;dbl&gt;\n\n\nSecond being the logit model.\n\nm1\n\n# A tibble: 3 × 14\n      p distance    df chi_square  pgof   slope slope_se slope_sig intercept\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1    10     503.     5      0.751 0.980 -0.0165   0.0155     0.287      6.08\n2    50     369.     5      0.751 0.980 -0.0165   0.0155     0.287      6.08\n3    90     236.     5      0.751 0.980 -0.0165   0.0155     0.287      6.08\n# ℹ 5 more variables: intercept_se &lt;dbl&gt;, intercept_sig &lt;dbl&gt;, z_value &lt;dbl&gt;,\n#   null_deviance &lt;dbl&gt;, aic &lt;dbl&gt;\n\n\nand third the probit model.\n\nm2\n\n# A tibble: 3 × 14\n      p distance    df chi_square  pgof    slope slope_se slope_sig intercept\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1    10     521.     5      0.864 0.973 -0.00815  0.00664     0.220      2.97\n2    50     364.     5      0.864 0.973 -0.00815  0.00664     0.220      2.97\n3    90     207.     5      0.864 0.973 -0.00815  0.00664     0.220      2.97\n# ℹ 5 more variables: intercept_se &lt;dbl&gt;, intercept_sig &lt;dbl&gt;, z_value &lt;dbl&gt;,\n#   null_deviance &lt;dbl&gt;, aic &lt;dbl&gt;\n\n\nConsidering the example data set is quite limited, you will notice each model performs differently, especially, the third-order polynomial which fits better than the logit and probit models. Each model predicts that at 50% detections will be heard at roughly 365-370 m away from the receiver."
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#plot-all-three-models",
    "href": "posts/post-with-code/detection-efficiency/index.html#plot-all-three-models",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "4.4 Plot all three models",
    "text": "4.4 Plot all three models\nWe will first plot the third order polynomial model first because there is a scaling issue as the third order polynomial is bound by 0-100 while the probit and logit models are bound by 0-1.\n\nggplot() +\n  geom_point(\n    data = sample_detection_efficiency,\n    aes(x = distance_m, y = avg_percent),\n    size = 3\n  ) +\n  geom_hline(yintercept = c(10, 50, 90), linetype = 2) +\n  geom_smooth(\n    data = sample_detection_efficiency,\n    aes(x = distance_m, y = avg_percent),\n    method = \"lm\",\n    linewidth = 1,\n    formula = y ~ -1 + x + I(x^2) +\n      I(x^3),\n    method.args = list(offset = sample_detection_efficiency$intercept),\n    colour = \"#8da0cb\", se = FALSE\n  ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  theme_bw(base_size = 15) +\n  theme(\n    panel.grid = element_blank()\n  ) +\n  labs(\n    x = \"Distance (m)\",\n    y = \"Detection efficency (%)\"\n  )\n\n\n\n\n\n\n\n\nI have added in the dotted lines at where the 10, 50, and 90% detection efficiency occurs. We will then plot the logit and probit models next.\n\nggplot() +\n  geom_point(\n    data = sample_detection_efficiency,\n    aes(x = distance_m, y = avg_percent_d),\n    size = 3\n  ) +\n  geom_hline(yintercept = c(0.10, 0.50, 0.90), linetype = 2) +\n  geom_smooth(\n    data = sample_detection_efficiency,\n    aes(x = distance_m, y = avg_percent_d),\n    method = \"glm\",\n    linewidth = 1,\n    method.args = list(family = binomial(link = \"logit\")),\n    colour = \"#66c2a5\", se = FALSE\n  ) +\n  geom_smooth(\n    data = sample_detection_efficiency,\n    aes(x = distance_m, y = avg_percent_d),\n    method = \"glm\",\n    linewidth = 1,\n    method.args = list(family = binomial(link = \"probit\")),\n    colour = \"#fc8d62\", se = FALSE\n  ) +\n  scale_y_continuous(breaks = seq(0, 1, 0.20)) +\n  theme_bw(base_size = 15) +\n  theme(\n    panel.grid = element_blank()\n  ) +\n  labs(\n    x = \"Distance (m)\",\n    y = \"Detection efficency (%)\"\n  )\n\n\n\n\n\n\n\n\nNow that we have our distances, we will need to create locations at the given percentages (e.g., 50%) we want to deploy for the course of the study. Given the above data we will deploy range transmitters at 365-370 m away from the receiver."
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#create-buffer-ring",
    "href": "posts/post-with-code/detection-efficiency/index.html#create-buffer-ring",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "5.1 Create buffer ring",
    "text": "5.1 Create buffer ring\nThe first step in redeployment of the range transmitters is to calculate a buffer ring at the given distance estimated from the models (e.g., 370 m). We will use steps very similar to the deployment steps.\n\nredeploy_loc &lt;- st_buffer(dist = 370, rcv_osc_sf_12) %&gt;%\n  st_cast(\"LINESTRING\") %&gt;%\n  mutate(\n    distance = 370\n  ) %&gt;%\n  dplyr::select(distance, glatos_array, station_no, ins_serial_no, geometry)\n# now view redeployment rings\nmapview(rcv_osc_sf) +\n  mapview(redeploy_loc)\n\n\n\n\n\nNow that we have the redeployment ring, we are going to pick 3 locations to potentially deploy range transmitters for the study period. We will then create an excel and gpx file of all the locations.\nFirst we transform our buffer rings into spatial points, view them using mapview() and filter out the points we want. Then save as an excel and/or gpx file.\n\nredeploy_loc_pts &lt;- redeploy_loc %&gt;%\n  st_cast(\"POINT\") %&gt;%\n  mutate(\n    id = 1:nrow(.)\n  ) %&gt;%\n  dplyr::select(id, distance:geometry)\n\nView them in mapview, this part will might take a little bit to select each point/figure out which ones you want.\n\nmapview(rcv_osc_sf) +\n  mapview(redeploy_loc_pts)"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#select-redeployment-location",
    "href": "posts/post-with-code/detection-efficiency/index.html#select-redeployment-location",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "5.2 Select redeployment location",
    "text": "5.2 Select redeployment location\nThen filter out the points we want, this will change depending on your study site and what locations you want. We will transform the projection back to WGS 84 as this is likely what your gps or sonar will want. We will also add in a few columns to conform to GLATOS and OTN data standards and rearrange the column order.\n\nredeploy_sites &lt;- redeploy_loc_pts %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  filter(id %in% c(116, 161, 201)) %&gt;%\n  rename(\n    receiver_serial_no = ins_serial_no\n  ) %&gt;%\n  mutate(\n    deploy_date_time = NA,\n    deploy_lat = NA,\n    deploy_long = NA,\n    bottom_depth = NA,\n    riser_length = NA,\n    instrument_depth = NA,\n    ins_model_number = NA,\n    ins_serial_no = NA,\n    transmitter = NA,\n    transmitter_model = NA,\n    deployed_by = NA,\n    recovered = NA,\n    recover_date_time = NA,\n    recover_lat = NA,\n    recover_long = NA,\n    data_downloaded = NA,\n    download_date_time = NA,\n    comments = NA,\n    expect_deploy_lat = st_coordinates(.)[, \"Y\"],\n    expect_deploy_long = st_coordinates(.)[, \"X\"],\n    comments = NA\n  ) %&gt;%\n  dplyr::select(\n    id:receiver_serial_no,\n    deploy_date_time:expect_deploy_long, geometry\n  )"
  },
  {
    "objectID": "posts/post-with-code/detection-efficiency/index.html#export-as-an-excel-and-gpx-formats",
    "href": "posts/post-with-code/detection-efficiency/index.html#export-as-an-excel-and-gpx-formats",
    "title": "Estimate detection efficiency for acoustic telemetry receivers",
    "section": "5.3 Export as an excel and gpx formats",
    "text": "5.3 Export as an excel and gpx formats\nThen we will save as an excel and gpx file. To save as excel we will use {openxlsx}. You will notice that I don’t have it in the load packages area of this vignette. I have not placed it there for the purpose of how R builds vignettes but please add it to your load packages call. You will also need to replace \"YOUR_FILE_PATH\" with your file path for both saving as an excel and/or gpx file.\n\n# drop geometry (coerce to data.frame)\nredeploy_sites_df &lt;- st_drop_geometry(redeploy_sites)\n\n# write xlsx file\nopenxlsx::write.xlsx(redeploy_sites_df, \"YOUR_FILE_PATH2.xlsx\")\n\n# save as gpx\nst_write(redeploy_sites, \"YOUR_FILE_PATH2.gpx\",\n  driver = \"GPX\",\n  dataset_options = \"GPX_USE_EXTENSIONS=YES\"\n)\n\nNow that you’ve created the excel and gpx file you can head out in the field to deploy your ranges transmitters for your study period. After the time period you will retriever your receivers, download the vrl files and create detection csvs. From there you can filter your range transmitters, calculate the number of heard in a day (e.g., 38) and divide it by the number you’re supposed to hear (e.g., 96), to get your daily detection efficiency. You can then model changes in daily detection efficiency over the course of the study. Congratulations! You have now successfully calculated your receiver detection range over your study time period."
  },
  {
    "objectID": "posts/post-with-code/soapcheckr/index.html",
    "href": "posts/post-with-code/soapcheckr/index.html",
    "title": "How to use {soapcheckr} to make soap-flim smoothers",
    "section": "",
    "text": "0.1 Our Objectives\nThe purpose of this vignette is to demonstrate an effective workflow while using {soapcheckr} to efficiently make a soap-film smoother for a Generalized Additive Model (GAM) using the package {mgcv}. Soap-film smoothers are really useful when trying to model a variable within a 3-dimensional space (e.g., bathymetry of a lake; Gavin Simpson Blog Post). They can be used for all sorts of data but are quite complex and difficult to setup. {soapcheckr} tries to make this process a little easier. This vignette will walk through several different examples using example data sets loaded when loading {soapcheckr} and {mgcv}. We strongly encourage going through both examples, as the first provides a general background on soap-films within a simple boundary and the second a more complex example within a more complex boundary.\n\n\n0.2 Example 1: Making a soap-film smoother for a Simple boundary - Ramsay’s horseshoe\nIf we were wanting to make a soap-film smoother for a boundary that does not have any inner boundaries (e.g., a lake without an island) we can use {soapcheckr} to assess our single boundary and the potential knots we want to smooth over.\n\n0.2.1 Install {soapcheckr}\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"dill/soapcheckr\")\n\n\n\n0.2.2 Load packages\nWe will use some data manipulation functions from {dplyr} and some spatial functions from {sf} to first check if we can make a soap-film using {soapcheckr} and second to run a GAM using {mgcv} with a soap-film smoother.\n\n{\n  library(broom.mixed)\n  library(fitdistrplus)\n  library(dplyr)\n  library(ggplot2)\n  library(gratia)\n  library(mgcv)\n  library(purrr)\n  library(soapcheckr)\n  library(sf)\n}\n\n\n\n0.2.3 Check if we can make a soap-film for Ramsey’s horseshoe\nWe will load Ramsey’s horseshoe from {mgcv} and make it a list within a list to sufficiently create the boundary a soap-film smoother needs within a GAM.\n\nfsb &lt;- list(fs.boundary())\n\nWe can then can check the boundary using soap_check().\n\nsoap_check(fsb)\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nWe can see that soap_check() returns TRUE indicating that we can use this boundary to make a soap-film smoother. soap_check() will assess if the boundary supplied, has any overlapping polygons and is in the correct structure for a soap-film smoother to be run in {mgcv}.\n\n\n0.2.4 Check if the data and the evenly spaced knots fall within the boundary\nSometimes knots are too close to the boundary, resulting with errors from the model that look like this:\nError in crunch.knots(ret$G, knots, x0, y0, dx, dy) :\nknot 1 is on or outside boundary\nIt can be tedious and annoying to try to figure out which knots and/or data points are causing issues. The below workflow will demonstrate how to effectively remove knots and/or data points that result in the above error within the GAM.\nWe will use expand.grid() to create an equally spaced grid of knots. Then we remove knots outside of the boundary using inSide() from {mgcv}. There still may be knots that are too close to the boundary that will cause for a soap-film smoother to not work.\n\n# create knots \nknots &lt;- expand.grid(x = seq(min(fsb[[1]]$x), \n                             max(fsb[[1]]$x), len = 15),\n                     y = seq(min(fsb[[1]]$y) + 0.05,\n                             max(fsb[[1]]$y), len = 10))\nx &lt;- knots$x\ny &lt;- knots$y\n\n# identify the knots that are outside the boundary \nind &lt;- inSide(fsb, x = x, y = y)\n# remove knots outside the boundary \nknots &lt;- knots[ind, ]\n\nWe will also create some fake data to test the model. We will use a uniform distribution to make our test data and a response variable will be created using fs.test(). We will use inSide() to remove data that fall outside our boundary.\n\nset.seed(0)\nn &lt;- 600\n\n# Our x and y data \nx &lt;- runif(n) * 5 - 1\ny &lt;- runif(n) * 2 - 1\n\n# create our response variable \nz &lt;- fs.test(x, y, b = 1)\n\n## remove outsiders\n\nind &lt;- inSide(fsb, x = x, y = y) \n\nz &lt;- z + rnorm(n) * 0.3 ## add noise\n\n# create the data we want to model \ndat &lt;- data.frame(z = z[ind],\n                  x = x[ind],\n                  y = y[ind])\n\nHowever, there still may be knots and/or data points that are too close to the boundary. One can go through, one-by-one and remove the offending knots as crunch.knots() finds them, but that’s a bit tedious. Enter soap_check() and autocruncher(), with the former allowing one to visually check what knots and/or data will causes issues and the latter identifying the index location of the offending knots or data points. If the knot dataframe has column names other than x and y, we need to supply soapcheckr() those names using the arguments x_name and y_name, respectfully.\n\nsoap_check(fsb, knots = knots)\n#&gt; Warning in soap_check(fsb, knots = knots): Knots 1, 13, 66, 93 are outside the\n#&gt; boundary.\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nWe can see that there four offending knots that we can subsequently remove using autocruncher(). This function will return the indices of the knots that would cause issues. If the knot dataframe has column names other than x and y, we need supply those column names to the arguments xname and yname, respectfully. Note that you need to set the k and nmax arguments in autocruncher() to be the same as your planned value in gam().\n\ncrunch_index &lt;- autocruncher(fsb, knots, k = 30)\ncrunch_index\n#&gt; [1]  1 13 66 93\n\n# remove knots that are problematic\nknots &lt;- knots[-crunch_index, ] \n\nWe can use soap_check() again to check if knots all fall within the boundary.\n\nsoap_check(fsb, knots = knots)\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nAnd they do! Congratulations!\n\n\n0.2.5 Check the data\nWe can also use soap_check() to check if our data falls within the boundary, but soap_check() only cares about the coordinates you want to supply the soap-film. So first we will create a secondary dataframe that has the response variable, z, removed from it.\n\ndat_2 &lt;- dat[, 2:3]\nsoap_check(fsb, data = dat_2)\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nCongrats we can see our data doesn’t fall too closely to boundary and is within the boundary.\n\n\n0.2.6 Run the model\nBetween data, knots, and boundary all column names must be the same for the model to work. Prior to running confirm that they are all the same.\n\nm &lt;- gam(z ~ s(x, y, k = 30 , bs = \"so\",\n               xt = list(bnd = fsb)),\n         knots = knots, \n         data = dat)\n\nNext, check main effects of the model.\n\nanova(m)\n\nThen visually check the model effects using draw() from {gratia}\n\ndraw(m)\n\n\n\n\n\n\n\n\nLastly, check the model fit using appraise() from {gratia}\n\nappraise(m)\n\n\n\n\n\n\n\n\nWe can see the model fits well and is appropriate for the example data. Now that you’ve walked through a simple boundary, we will go to a more complex boundary example. More than likely you will be working with a complex boundary. The below walk through can be applied to a simple boundary as well.\n\n\n\n0.3 Example 2: Making a soap-film smoother for a more complex boundary.\nMore than likely you will have your boundary as a sf object. To convert that sf object into the boundary list needed by {mgcv} and {soapcheckr}, we will have to do some conversions. First that boundary might not be in the correct coordinate reference system (CRS). To create a soap-film smoother we need to use a CRS that for one unit change in either dimension (i.e., x and y) are equal. For example using latitude and longitude in decimal degrees with a WGS 84 projection will not work because one unit of change in either direction is not equal. Therefore, we will need to use a CRS that is based on equal units. The most common CRS to do this is UTMs, if the boundary is already in UTMs great, if not see below.\n\n0.3.1 Convert CRS\nThe more complex boundary that we will load through {soapcheckr} is a lake from northern Wisconsin, Sissabagama Lake. I grew up fishing on this lake which is where initially became interested and passionate about aquatic ecosystems, fish, and fisheries management. This lake falls within UTM zone 15N that can also be refereed to as ESPG: 32615, but your boundary will more than likely fall into a different CRS. You can look up ESPG codes here.\n\nsissabagama_lake_sf &lt;- sissabagama_lake_sf %&gt;% \n  st_transform(crs = 32615)\n\n\n\n0.3.2 Convert to boundary list\nWe need to create the list of lists of the boundaries from the sf object that we will supply to the soap-film smoother.\nOur example lake has a geometry column that is a POLYGON. We need to be able to split that into each polygon (i.e., islands) that we will create the boundary list from. We can do this by first casting our geometry into MULTIPOINT and assigning each MULTIPOINT row an ID value.\n\nbnd_pt_sf &lt;- sissabagama_lake_sf %&gt;%\n  dplyr::select(geometry) %&gt;%\n  st_cast(\"MULTIPOINT\") %&gt;%\n  mutate(\n    id = 1:nrow(.)\n  )\n\nNext we will split our sf object and iterate over each MULTIPOINT geometry to first cast to individual POINT geometry and extract each x and y coordinates. It is important in this step that the names of the coordinates are x and y.\n\nbnd_pt &lt;- bnd_pt_sf %&gt;%\n  split(.$id) %&gt;%\n  purrr::map(~ st_cast(.x, \"POINT\") %&gt;%\n               mutate(\n                 x = st_coordinates(.)[,\"X\"],\n                 y = st_coordinates(.)[,\"Y\"]\n               ) %&gt;%\n               st_drop_geometry() %&gt;% \n               dplyr::select(-id)\n  )\n\nWe now have a list of dataframes split by each polygon’s x and y coordinates that have had the id column removed. We then need to create a vector that is id number of each polygon. In this case it’s 1-5, we can use length() of our list of dataframes to easily create the end of our numerical vector.\nWe will then iterate over our list and bind them all together to get our lists and lists of our polygon boundaries.\n\nnr &lt;- 1:length(bnd_pt)\n\nsissabagama_bnd_ls &lt;- lapply(nr, function(n) as.list.data.frame(bnd_pt[[n]]))\n\n\n\n0.3.3 Check if the boundary list works\nWe will check the boundary list using soap_check()\n\nsoap_check(sissabagama_bnd_ls)\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nsoap_check returns back TRUE so our more complex boundary will work for our soap-film smoother. One thing that I’ve always loved about this lake is how it looks like person!\n\n\n0.3.4 Make knots for a more complex boundary using {sf}\nWe can use st_make_grid() to create a grid of equally spaced points across the boundary box of our example sf object of Sissabagama Lake. Remember a soap-film smoother needs equally spaced knots to smooth over. Our sf object is in UTMs which is great becasue then each grid point in this case is 200 m away from each other. Depending on the size of the boundary and system you can change 200 to whatever value makes sense (e.g., large system, further spaced knots, small system, closer spaced knots/this is suggestion but do whatever makes sense).\n\nlake_grid &lt;- sissabagama_lake_sf %&gt;%\n  st_make_grid(cellsize = 200, square = TRUE, what = \"centers\") %&gt;%\n  st_as_sf() \n\nst_geometry(lake_grid) &lt;- \"geometry\"\n\nWe will then remove all the knots that fall outside the boundary by using st_intersection().\n\nlake_intesects &lt;- st_intersection(sissabagama_lake_sf, lake_grid)\n\nNext we will create our knot dataframe by extracting the lon and lat of each point and then dropping the geometry column and selecting our lon and lat columns.\n\nlake_knots &lt;- lake_intesects %&gt;%\n  mutate(\n    lon = st_coordinates(.)[,\"X\"],\n    lat = st_coordinates(.)[,\"Y\"]\n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::select(lon, lat)\n\nNow that we have our knots we can check to see if there are any knots that fall too close to the boundary using soap_check().\n\nsoap_check(sissabagama_bnd_ls, knots = lake_knots, \n           x_name = \"lon\", y_name = \"lat\")\n#&gt; Warning in soap_check(sissabagama_bnd_ls, knots = lake_knots, x_name = \"lon\", :\n#&gt; Knots 42, 63, 68 are outside the boundary.\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nWe can see that there are a few knots that are too close to the boundary. We can remove them using autocruncher()\n\ncrunch_ind &lt;- autocruncher(sissabagama_bnd_ls, lake_knots, \n                           xname = \"lon\", yname = \"lat\")\ncrunch_ind\n#&gt; [1] 42 63 68\n\n# remove knots that are problematic\nlake_knots &lt;- lake_knots[-crunch_ind, ] \n\nNow that those knots have been removed we can recheck our knots using soap_check().\n\nsoap_check(sissabagama_bnd_ls, knots = lake_knots, \n           x_name = \"lon\", y_name = \"lat\")\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nCongratulations! We have knots and a boundary that we can supply our model.\n\n\n0.3.5 Sissabagama example dataset\nWe will bring our sampled depths at given locations for Sissabagama lake. This data was generated by referencing the contour map supplied by the Wisconsin DNR. We will first remove depth to check if the data points all fall within our boundary using soap_check().\n\nsissabagama_bath_pt &lt;- sissabagama_bath %&gt;% \n  dplyr::select(-depth)\n\nsoap_check(sissabagama_bnd_ls, data = sissabagama_bath_pt)\n\n\n\n\n\n\n\n#&gt; [1] TRUE\n\nThen we will assess the distribution of the data to determine which distribution the model should use to fit the data to. We will use functions from {fitdistrplus}.\n\ndepths &lt;- sissabagama_bath$depth\ndescdist(depths)\n\n\n\n\n\n\n\n#&gt; summary statistics\n#&gt; ------\n#&gt; min:  3   max:  48 \n#&gt; median:  15 \n#&gt; mean:  15.29438 \n#&gt; estimated sd:  9.983594 \n#&gt; estimated skewness:  0.7078068 \n#&gt; estimated kurtosis:  2.964066\n\nSkewness and kurtosis of our example data indicates that a model using a Gamma error distribution will likely fit.\n\nfit_gamma &lt;- fitdist(depths, distr = \"gamma\", method = \"mme\")\nplot(fit_gamma)\n\n\n\n\n\n\n\n\nWe can see the depth data will likely fit a Gamma error distribution and therefore our GAM will use a Gamma error distribution.\n\n\n0.3.6 Run the model\nPrior to running our GAM with a soap-film smoother we need to add one last thing to our boundary list. We need to add the variable f to every boundary polygon within our boundary list. This variable indicates to the soap-film smoother that our response variable is 0 right at the boundary, otherwise the soap-film smoother does not know what to do when it hits the boundary.\n\nnames(lake_knots) &lt;- c(\"x\", \"y\")\n\nsissabagama_bnd_ls &lt;- lapply(nr,\n                             function(n)\n                               sissabagama_bnd_ls[[n]] &lt;- c(\n                                 sissabagama_bnd_ls[[n]],\n                                 list(f = rep(0, length(sissabagama_bnd_ls[[n]]$x))\n                                 )\n                               )\n)\n\nWe can now successfully run our GAM with a soap-film smoother.\n\nm1 &lt;- gam(depth ~ s(x, y,\n                    bs = \"so\",\n                    xt = list(bnd = sissabagama_bnd_ls)),\n          family = Gamma(link = \"identity\"),\n          knots = lake_knots,\n          data = sissabagama_bath)\n\nWe can evaluate the main of the model using anova()\n\nanova(m1)\n#&gt; \n#&gt; Family: Gamma \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; depth ~ s(x, y, bs = \"so\", xt = list(bnd = sissabagama_bnd_ls))\n#&gt; \n#&gt; Approximate significance of smooth terms:\n#&gt;          edf Ref.df     F p-value\n#&gt; s(x,y) 57.47  76.00 16.51  &lt;2e-16\n\nNext we can evaluate partial effects of the model using summary()\n\nsummary(m1)\n#&gt; \n#&gt; Family: Gamma \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; depth ~ s(x, y, bs = \"so\", xt = list(bnd = sissabagama_bnd_ls))\n#&gt; \n#&gt; Parametric coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   2.8408     0.2074    13.7   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Approximate significance of smooth terms:\n#&gt;          edf Ref.df     F p-value    \n#&gt; s(x,y) 57.47     76 16.51  &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; R-sq.(adj) =  0.786   Deviance explained = 76.8%\n#&gt; GCV = 0.15835  Scale est. = 0.12696   n = 445\n\nWe can evaluate the partial effects of the model using draw() from {gratia}\n\ndraw(m1)\n\n\n\n\n\n\n\n\nWe can evaluate how well the model fit is to the data using appraise() also from {gratia}\n\nappraise(m1)\n\n\n\n\n\n\n\n\n\n\n0.3.7 Plot our predicted results\nWe will first create a 10 m grid from our sf object of our boundary. Depending on the size of the boundary you can change the grid size distance to whatever distance makes sense (i.e., if the system is large you may want to increase the cellsize).\n\nlake_pred &lt;- sissabagama_lake_sf %&gt;%\n  st_make_grid(cellsize = 10, square = TRUE, what = \"centers\") %&gt;% \n  st_as_sf() \nst_geometry(lake_pred) &lt;- \"geometry\"\n\nAfter creating the grid that we will predict values from we will need to remove any points that fall outside our polygon boundary.\n\nlake_pred &lt;- st_intersection(lake_pred, sissabagama_lake_sf) %&gt;% \n  dplyr::select(geometry)\n\nThen we extract latitude and longitude and convert the sf object into a dataframe.\n\nlake_pred_df &lt;- lake_pred %&gt;% \n  mutate(\n    x = st_coordinates(.)[,\"X\"], \n    y = st_coordinates(.)[,\"Y\"], \n  ) %&gt;% \n  st_drop_geometry()\n\nWe then can use the function augment() from the package {broom.mixed} to predict depth of the lake at a given latitude and longitude.\n\npred &lt;- augment(m1, newdata = lake_pred_df)\npred &lt;- pred %&gt;% \n  mutate(\n    lower = .fitted - 1.96 * .se.fit,\n    higher = .fitted + 1.96 * .se.fit\n  )\n\nLastly, we can visualize our predicted depths to create a bathymetic map of the lake using ggplot().\n\nggplot() +\n  geom_raster(data = pred, aes(x = x, y = y, fill = .fitted)) +\n  geom_sf(data = sissabagama_lake_sf, fill = NA, colour = \"black\") +\n  scale_fill_viridis_c(name = \"Depth (m)\",\n                       trans = \"reverse\",\n                       breaks = rev(seq(0, 60, 15))\n  ) + \n  theme_void(\n    base_size = 15\n  ) + \n  theme(\n    legend.background = element_blank(),\n    legend.position = c(0.98, 0.82),\n  ) + \n  guides(fill = guide_colourbar(\n    frame.linewidth = 0.3,\n    ticks.colour = 'black', \n    frame.colour = 'black')) + \n  labs(x = \"Longitude\", \n       y = \"Latitude\")\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\nCongratulations! We have made a soap-film GAM that takes in account the boundaries within the lake to estimate the bathymetry of the lake. For this example I used bathymetry, but you can use this workflow to model any type of variable (e.g., fish and/or animal movement/acceleration, wind speed, water quality, ect.)"
  }
]